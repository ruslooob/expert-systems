{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:10:50.993279400Z",
     "start_time": "2023-12-17T11:10:50.777623800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Загрузка обучающей и тестовой выборки"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d77a5ebb854176d"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "\n",
    "def get_train_data(categories):\n",
    "    if type(categories) is not list:\n",
    "        categories = [categories]\n",
    "    return fetch_20newsgroups(subset='train', shuffle=True, categories=categories, random_state=42, remove=remove)\n",
    "\n",
    "\n",
    "all_categories = ['comp.graphics', 'sci.crypt', 'sci.electronics']\n",
    "train_bunch = get_train_data(all_categories)\n",
    "test_bunch = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, categories=all_categories, remove=remove)\n",
    "\n",
    "\n",
    "def get_sample(bunch, category_idx):\n",
    "    for idx, target in enumerate(bunch.target):\n",
    "        if target == category_idx:\n",
    "            return bunch.data[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:10:52.544521200Z",
     "start_time": "2023-12-17T11:10:50.794165800Z"
    }
   },
   "id": "9eae428c0f3ee6d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "4. Вывод по одному документа каждого из классов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64f88417caaa408a"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Hello, I realize that this might be a FAQ but I have to ask since I don't get a\\nchange to read this newsgroup very often.  Anyways for my senior project I need\\nto convert an AutoCad file to a TIFF file.  Please I don't need anyone telling\\nme that the AutoCAD file is a vector file and the TIFF is a bit map since I\\nhave heard that about 100 times already I would just like to know if anyone\\nknows how to do this or at least point me to the right direction.\""
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample(train_bunch, all_categories.index('comp.graphics'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:10:52.553229200Z",
     "start_time": "2023-12-17T11:10:52.544521200Z"
    }
   },
   "id": "ac9222ee529c5311"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "'Looking for PostScript or Tex version of a paper called:\\n\\t\"PUBLIC-KEY CRYPTOGRAPHY\"\\n\\nWritten by:\\n\\tJames Nechvatal\\n\\tSecurity Technology Group\\n\\tNational Computer Systems Laboratory\\n\\tNational Institute of Standards and Technology\\n\\tGaithersburg, MD 20899\\n\\n\\tDecember 1990\\n\\nThe version I obtained is plain text and all symbolic character\\nformatting has been lost.\\n'"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample(train_bunch, all_categories.index('sci.crypt'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:10:52.564381800Z",
     "start_time": "2023-12-17T11:10:52.550192Z"
    }
   },
   "id": "95df8a2025d87bb9"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "'Just a thought........Maybe it possibly has to do with the fact that it\\nIS an Emerson.  I\\'ve got an Emerson VCR which is #6 in the series.  Returned\\nit six times for various and never the same problems.  Got tired of taking it \\nback and fixed it myself.  The Hi-Fi \"window\" was a bit off.  Something like\\nthe Hi-Fi audio fine-tuning.  When I was a Wal-Mart \"associate\" in \\'88-\\'89,\\nwe had AT LEAST one returned as defective EVERY SINGLE DAY.  How\\'s that for\\nreliability?  Face it--Emerson can make audio stuff (albeit not of premium\\nquality), but they CAN\\'T make anything as complex as video equipment with \\nreliability IMHO.  Please, no flames.  Just *had* to share my Emerson disaster\\nin the light of this exploding tv.  \\nJC\\n\\n\\n'"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample(train_bunch, all_categories.index('sci.electronics'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:10:52.568771500Z",
     "start_time": "2023-12-17T11:10:52.564381800Z"
    }
   },
   "id": "b80e7a3e75954a9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Выполнение процедуры стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c355e8bef14d528"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ruslan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def stemminize(documents: list[str]) -> list[str]:\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stem_train = []\n",
    "    for document in documents:\n",
    "        nltk_tokens = word_tokenize(document)\n",
    "        line = ''\n",
    "        for word in nltk_tokens:\n",
    "            line += ' ' + porter_stemmer.stem(word)\n",
    "        stem_train.append(line)\n",
    "    return stem_train\n",
    "\n",
    "\n",
    "train_tokenized = stemminize(train_bunch.data)\n",
    "test_tokenized = stemminize(test_bunch.data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:00.207805500Z",
     "start_time": "2023-12-17T11:10:52.568771500Z"
    }
   },
   "id": "7b040729d22d126f"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "[\" hello , i realiz that thi might be a faq but i have to ask sinc i do n't get a chang to read thi newsgroup veri often . anyway for my senior project i need to convert an autocad file to a tiff file . pleas i do n't need anyon tell me that the autocad file is a vector file and the tiff is a bit map sinc i have heard that about 100 time alreadi i would just like to know if anyon know how to do thi or at least point me to the right direct .\",\n \" just a thought ........ mayb it possibl ha to do with the fact that it is an emerson . i 've got an emerson vcr which is # 6 in the seri . return it six time for variou and never the same problem . got tire of take it back and fix it myself . the hi-fi `` window '' wa a bit off . someth like the hi-fi audio fine-tun . when i wa a wal-mart `` associ '' in '88-'89 , we had at least one return as defect everi singl day . how 's that for reliabl ? face it -- emerson can make audio stuff ( albeit not of premium qualiti ) , but they ca n't make anyth as complex as video equip with reliabl imho . pleas , no flame . just * had * to share my emerson disast in the light of thi explod tv . jc\",\n \" it 's realli not that hard to do . there are book out there which explain everyth , and the basic 3d function , translat , rotat , shade , and hidden line remov are pretti easi . i wrote a program in a few week witht he help of a book , and would be happi to give you my sourc . also , quickdraw ha a lot of 3d function built in , and think pascal can access them , and i would expect that think c could as well . if you can find out how to use the quickdraw graphic librari , it would be an excel choic , sinc it ha a lot of stuff , and is built into the mac , so should be fast . libertarian , atheist , semi-anarch techno-rat .\"]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вывод 3 первых документов обучающих данных\n",
    "train_tokenized[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:00.214131Z",
     "start_time": "2023-12-17T11:11:00.207805500Z"
    }
   },
   "id": "82514f4345adc953"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "[' well , i am place a file at my ftp today that contain sever polygon descript of a head , face , skull , vase , etc . the format of the file is a list of vertic , normal , and triangl . there are variou resolut and the name of the data file includ the number of polygon , eg . phred.1.3k.vbl contain 1300 polygon . in order to get the data via ftp do the follow : 1 ) ftp taurus.cs.nps.navy.mil 2 ) login as anonym , guest as the password 3 ) cd pub/dabro 4 ) binari 5 ) get cyber.tar.z onc you get the data onto your workstat : 1 ) uncompress data.tar.z 2 ) tar xvof data.tar if you have ani question , pleas let me know . georg dabro dabro @ taurus.cs.nps.navy.mil -- georg dabrowski cyberwar lab',\n \" tri search for dmorf , i think it 's locat on wuarchive.wustl.edu in a mirror directori ... i 've use it befor , & it wa pretti good !\",\n ' not realli . i think it is less than 10 % .']"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вывод 3 первых документов тестовых данных\n",
    "test_tokenized[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:00.216025600Z",
     "start_time": "2023-12-17T11:11:00.211994700Z"
    }
   },
   "id": "6b4cec2df528fe76"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "columns = pd.MultiIndex.from_product([['Count', 'TF', 'TF-IDF'], ['Без стоп-слов', 'С стоп-словами']])\n",
    "df_train = pd.DataFrame(columns=columns)\n",
    "df_test = pd.DataFrame(columns=columns)\n",
    "\n",
    "df_train_stem = pd.DataFrame(columns=columns)\n",
    "df_test_stem = pd.DataFrame(columns=columns)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:00.223126700Z",
     "start_time": "2023-12-17T11:11:00.216025600Z"
    }
   },
   "id": "338b35b9b40014b8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6 Векторизация и вывод 20 наиболее частых слов для всей тренировочной выборки без стоп-слов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "357cb1d9af7f853a"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 16689),\n ('to', 8883),\n ('of', 7021),\n ('and', 6843),\n ('is', 5467),\n ('in', 4416),\n ('it', 3900),\n ('that', 3682),\n ('for', 3677),\n ('you', 2852),\n ('be', 2788),\n ('this', 2585),\n ('on', 2451),\n ('are', 2155),\n ('with', 2111),\n ('or', 2090),\n ('have', 1879),\n ('as', 1784),\n ('can', 1704),\n ('if', 1702)]"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000)\n",
    "train_data = vect.fit_transform(train_bunch.data)\n",
    "\n",
    "\n",
    "def get_20_freq_words(vect, data):\n",
    "    words = list(zip(vect.get_feature_names_out(), np.ravel(data.sum(axis=0))))\n",
    "    words.sort(key=lambda x: x[1], reverse=True)\n",
    "    return words[:20]\n",
    "\n",
    "\n",
    "count_column = get_20_freq_words(vect, train_data)\n",
    "df_train['Count', 'Без стоп-слов'] = count_column\n",
    "count_column"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:00.407561800Z",
     "start_time": "2023-12-17T11:11:00.223126700Z"
    }
   },
   "id": "506302ae4212e71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6 Векторизация и вывод 20 наиболее частых слов для всей тренировочной выборки со стоп-словами"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb9f331ad289ef96"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "[('key', 937),\n ('use', 932),\n ('like', 642),\n ('don', 592),\n ('db', 562),\n ('edu', 553),\n ('encryption', 552),\n ('data', 547),\n ('know', 542),\n ('just', 533),\n ('chip', 521),\n ('does', 501),\n ('used', 498),\n ('information', 497),\n ('image', 492),\n ('people', 483),\n ('time', 447),\n ('bit', 437),\n ('file', 427),\n ('graphics', 423)]"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vect.fit_transform(train_bunch.data)\n",
    "\n",
    "count_column_stop = get_20_freq_words(vect, dtm)\n",
    "df_train['Count', 'С стоп-словами'] = count_column_stop\n",
    "count_column_stop"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:00.577606300Z",
     "start_time": "2023-12-17T11:11:00.410854100Z"
    }
   },
   "id": "19ab0f55db9071f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью TfidfTransformer без стоп-слов TF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b3c57f6c668850c"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 573.5878746017531),\n ('to', 323.08392380711285),\n ('of', 242.72988399270668),\n ('and', 217.11039200783313),\n ('is', 186.06314755308978),\n ('it', 169.68759255688352),\n ('in', 162.3235233209576),\n ('for', 148.54566211510095),\n ('that', 146.62595086524686),\n ('you', 120.10726765931362),\n ('this', 105.17732299778746),\n ('be', 98.40643227700879),\n ('on', 96.18830762929639),\n ('have', 86.45612956176703),\n ('with', 76.85987967685325),\n ('if', 75.80342481606581),\n ('or', 75.78439948121508),\n ('are', 74.94851602350174),\n ('can', 68.77692347543262),\n ('not', 63.1163398649122)]"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_20_freq_words_idf(feature_names, tfidf_values):\n",
    "    result = []\n",
    "    word_weights = dict(zip(feature_names, tfidf_values))\n",
    "    sorted_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, weight in sorted_words[:20]:\n",
    "        result.append((word, weight))\n",
    "    return result\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "dtm = vectorizer.fit_transform(train_bunch.data)\n",
    "tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_column = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_train['TF', 'Без стоп-слов'] = tf_column\n",
    "tf_column"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:00.811440800Z",
     "start_time": "2023-12-17T11:11:00.577606300Z"
    }
   },
   "id": "efa25d100bdec93f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью TfidfTransformer с стоп-словами TF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdf225c687c80609"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "[('use', 58.433522994302784),\n ('know', 57.307906073228544),\n ('like', 56.64353296005652),\n ('don', 49.839861337882176),\n ('just', 49.002983276208454),\n ('does', 48.9585366753105),\n ('key', 48.142224601803),\n ('thanks', 39.15072837325299),\n ('chip', 35.95980854400468),\n ('good', 35.08471270781613),\n ('need', 32.70371344296186),\n ('used', 31.85354729678519),\n ('think', 31.408220640281492),\n ('ve', 31.265005741849148),\n ('time', 30.354985689209872),\n ('people', 30.323719447911316),\n ('encryption', 28.319287928894738),\n ('using', 27.62980799728084),\n ('graphics', 27.205639758499185),\n ('clipper', 26.571725044843287)]"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(train_bunch.data)\n",
    "tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_column_stop = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_train['TF', 'С стоп-словами'] = tf_column_stop\n",
    "tf_column_stop"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:01.044749500Z",
     "start_time": "2023-12-17T11:11:00.811440800Z"
    }
   },
   "id": "b29dbe68cb61450d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью TfidfTransformer без стоп-слов TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a2be35db58986cc"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 214.27923011983643),\n ('to', 128.50658234239415),\n ('of', 101.86203264775209),\n ('and', 91.733961272254),\n ('is', 83.97727336110019),\n ('it', 79.47554322700573),\n ('in', 72.70953833589493),\n ('that', 72.51290798766651),\n ('for', 67.66475343892618),\n ('you', 66.41180952122423),\n ('be', 55.65313856163747),\n ('this', 55.13173979874598),\n ('on', 50.56554310512375),\n ('have', 48.11707358740201),\n ('are', 44.12242787539058),\n ('with', 43.883460758799835),\n ('if', 43.62474130570933),\n ('or', 43.39770046917348),\n ('can', 40.89942210826312),\n ('as', 40.345366794020045)]"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "dtm = vectorizer.fit_transform(train_bunch.data)\n",
    "tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_idf = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_train['TF-IDF', 'Без стоп-слов'] = tf_idf\n",
    "tf_idf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:01.289639800Z",
     "start_time": "2023-12-17T11:11:01.044749500Z"
    }
   },
   "id": "387def9a86f1221"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью TfidfTransformer со стоп-словами TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b4e33c8b0b4951c"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "[('key', 31.639250020331772),\n ('know', 29.275985507770525),\n ('use', 28.5909199903978),\n ('like', 27.36974112627459),\n ('does', 27.32020401912984),\n ('don', 25.935560274661245),\n ('just', 25.519485103509275),\n ('chip', 24.453605471872685),\n ('thanks', 24.11088770306822),\n ('encryption', 20.612197002323146),\n ('good', 20.345796034175873),\n ('need', 19.637524126929815),\n ('ve', 19.573641488242785),\n ('graphics', 19.457067951393185),\n ('clipper', 19.278143372470023),\n ('people', 18.825771055423083),\n ('think', 18.438571829249657),\n ('used', 18.158442956203725),\n ('government', 17.929344754138906),\n ('time', 17.5444284746565)]"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(train_bunch.data)\n",
    "tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_idf_stop = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_train['TF-IDF', 'С стоп-словами'] = tf_idf_stop\n",
    "tf_idf_stop"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:01.517514400Z",
     "start_time": "2023-12-17T11:11:01.289639800Z"
    }
   },
   "id": "f334676515430934"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Обработка тестовых данных"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bdb6f6904695bd1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6 Векторизация и вывод 20 наиболее частых слов для всей тестовой выборки без стоп-слов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37141f98e3dc0224"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 9066),\n ('to', 5360),\n ('of', 4137),\n ('and', 4073),\n ('is', 3074),\n ('in', 2610),\n ('it', 2402),\n ('for', 2362),\n ('that', 2228),\n ('you', 2086),\n ('be', 1535),\n ('this', 1472),\n ('on', 1462),\n ('or', 1295),\n ('with', 1258),\n ('have', 1215),\n ('are', 1186),\n ('if', 1154),\n ('can', 1101),\n ('as', 1026)]"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000)\n",
    "dtm = vect.fit_transform(test_bunch.data)\n",
    "count_test_column = get_20_freq_words(vect, dtm)\n",
    "df_test['Count', 'Без стоп-слов'] = count_test_column\n",
    "count_test_column"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:01.640140900Z",
     "start_time": "2023-12-17T11:11:01.517514400Z"
    }
   },
   "id": "59324e91ae81a54f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6 Векторизация и вывод 20 наиболее частых слов для всей тестовой выборки со стоп-словами"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a978614e03fa4813"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "[('image', 666),\n ('jpeg', 526),\n ('use', 516),\n ('edu', 468),\n ('graphics', 462),\n ('like', 408),\n ('file', 389),\n ('don', 378),\n ('data', 368),\n ('know', 355),\n ('just', 339),\n ('bit', 337),\n ('available', 325),\n ('software', 324),\n ('images', 307),\n ('program', 298),\n ('does', 291),\n ('time', 282),\n ('used', 272),\n ('ftp', 271)]"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vect.fit_transform(test_bunch.data)\n",
    "count_column_stop = get_20_freq_words(vect, dtm)\n",
    "df_test['Count', 'С стоп-словами'] = count_column_stop\n",
    "count_column_stop"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:01.753126500Z",
     "start_time": "2023-12-17T11:11:01.640140900Z"
    }
   },
   "id": "a3936e7fea3bc87d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для тестовой выборки с помощью TfidfTransformer без стоп-слов TF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43cb47571a9440c9"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 351.8833889205307),\n ('to', 215.21048224463186),\n ('of', 153.66848738794562),\n ('and', 140.95239513687198),\n ('is', 120.607868681005),\n ('it', 110.88904071825),\n ('in', 104.33536769410595),\n ('that', 98.57686193779719),\n ('for', 92.40323586413749),\n ('you', 81.3739413442512),\n ('be', 65.317483332802),\n ('on', 62.06126483104049),\n ('this', 61.59725664266683),\n ('have', 57.61931147617616),\n ('or', 51.777520899187046),\n ('if', 50.32542218561264),\n ('can', 49.21418012871439),\n ('with', 48.18761054093589),\n ('are', 45.11043625995436),\n ('not', 43.53696710250099)]"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "dtm = vectorizer.fit_transform(test_bunch.data)\n",
    "tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_column_test = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_test['TF', 'Без стоп-слов'] = tf_column_test\n",
    "tf_column_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:01.914888600Z",
     "start_time": "2023-12-17T11:11:01.751280100Z"
    }
   },
   "id": "c7bf2ab59777ddad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для тестовой выборки с помощью TfidfTransformer со стоп-словами TF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8735b8f64bb337e1"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "[('know', 42.06416144310403),\n ('like', 36.822127481402006),\n ('use', 36.792352685034714),\n ('just', 32.40544450511485),\n ('don', 30.469003399281732),\n ('does', 29.885181249520144),\n ('thanks', 27.29259689277586),\n ('think', 24.82104074312471),\n ('used', 21.459669060506144),\n ('need', 20.686196868337543),\n ('graphics', 20.679160827517396),\n ('time', 20.129142247758516),\n ('program', 19.584807797820034),\n ('people', 19.102579785729354),\n ('chip', 18.54157887888643),\n ('edu', 18.28349107306669),\n ('ve', 18.270863639958304),\n ('government', 18.089022419581863),\n ('good', 17.967957804096248),\n ('bit', 17.440629791897027)]"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(test_bunch.data)\n",
    "tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_column_stop_test = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_test['TF', 'С стоп-словами'] = tf_column_stop_test\n",
    "tf_column_stop_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:02.074464400Z",
     "start_time": "2023-12-17T11:11:01.914888600Z"
    }
   },
   "id": "addf75a5879dc34c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для тестовой выборки с помощью TfidfTransformer без стоп-слов TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1044e1ccae271c6e"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 132.73987348944183),\n ('to', 85.24139589140572),\n ('of', 64.8113206497389),\n ('and', 59.482599607653924),\n ('is', 54.52831275498656),\n ('it', 53.75496178917013),\n ('that', 47.770429081592496),\n ('in', 47.06683267580104),\n ('you', 45.42774190604445),\n ('for', 43.184971775965785),\n ('be', 35.43112523777322),\n ('this', 34.08603226381701),\n ('on', 32.90027716297289),\n ('have', 32.025491313098804),\n ('if', 29.12956299199104),\n ('or', 29.011357783500337),\n ('can', 28.82297538311695),\n ('are', 27.38799920531715),\n ('with', 27.03257285835745),\n ('not', 26.9403506133788)]"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "dtm = vectorizer.fit_transform(test_bunch.data)\n",
    "tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_idf_test = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_test['TF-IDF', 'Без стоп-слов'] = tf_idf_test\n",
    "tf_idf_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:02.220408800Z",
     "start_time": "2023-12-17T11:11:02.074464400Z"
    }
   },
   "id": "a0dbf4cea35de0bb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для тестовой выборки с помощью TfidfTransformer со стоп-словами TF-IDF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3194723ea9482339"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "[('know', 21.435150574064025),\n ('like', 18.950713675342715),\n ('use', 18.420212557191185),\n ('thanks', 17.098531494337998),\n ('does', 16.98361584501106),\n ('just', 16.684961307647363),\n ('don', 16.428887334276713),\n ('think', 14.557804515797576),\n ('graphics', 14.252209873941238),\n ('program', 13.64987318610155),\n ('government', 12.964530900368489),\n ('chip', 12.860296724296857),\n ('used', 12.439336643011975),\n ('people', 12.333813168768687),\n ('need', 12.240898928564446),\n ('bit', 12.015183440146776),\n ('edu', 11.906194792288247),\n ('ve', 11.870735103204861),\n ('time', 11.745018935759806),\n ('key', 11.670618169470233)]"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(test_bunch.data)\n",
    "tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_idf_stop_test = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_test['TF-IDF', 'С стоп-словами'] = tf_idf_stop_test\n",
    "tf_idf_stop_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:02.377602600Z",
     "start_time": "2023-12-17T11:11:02.220408800Z"
    }
   },
   "id": "8707f76fce76ee1d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Стемминг"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34adab493c00d390"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Тренировочные данные"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7862ca4ec5e93d8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью CountVectorizer без стоп-слов c приминением стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3dd938c86de963ad"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 16688),\n ('to', 8883),\n ('of', 7021),\n ('and', 6843),\n ('is', 5549),\n ('in', 4419),\n ('it', 4191),\n ('that', 3692),\n ('for', 3677),\n ('be', 2998),\n ('you', 2852),\n ('thi', 2585),\n ('on', 2459),\n ('are', 2195),\n ('with', 2111),\n ('or', 2090),\n ('use', 2014),\n ('have', 1997),\n ('as', 1784),\n ('not', 1740)]"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000)\n",
    "dtm = vect.fit_transform(train_tokenized)\n",
    "count_column_stem = get_20_freq_words(vect, dtm)\n",
    "df_train_stem['Count', 'Без стоп-слов'] = count_column_stem\n",
    "count_column_stem"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:02.541815400Z",
     "start_time": "2023-12-17T11:11:02.377602600Z"
    }
   },
   "id": "dfba560f854958a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью CountVectorizer с стоп-словами c приминением стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afeefd5b864a2450"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "[('thi', 2585),\n ('use', 2014),\n ('key', 1283),\n ('ha', 887),\n ('ani', 866),\n ('wa', 783),\n ('encrypt', 774),\n ('imag', 737),\n ('file', 730),\n ('like', 711),\n ('chip', 672),\n ('doe', 671),\n ('know', 622),\n ('bit', 621),\n ('program', 569),\n ('db', 562),\n ('onli', 560),\n ('edu', 553),\n ('data', 548),\n ('secur', 534)]"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vect.fit_transform(train_tokenized)\n",
    "count_column_stem_stop = get_20_freq_words(vect, dtm)\n",
    "df_train_stem['Count', 'С стоп-словами'] = count_column_stem_stop\n",
    "count_column_stem_stop"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:02.701120100Z",
     "start_time": "2023-12-17T11:11:02.541815400Z"
    }
   },
   "id": "9aaab58eb16e7e41"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью TfidfTransformer без стоп-слов c приминением стемминга TF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abc549c36db02d5d"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 561.4880559369901),\n ('to', 316.3652287700625),\n ('of', 237.41198848231448),\n ('and', 212.34202585341927),\n ('is', 185.57446689326932),\n ('it', 175.55160439683934),\n ('in', 159.25693721881674),\n ('for', 145.53831889691298),\n ('that', 144.01545732916136),\n ('you', 117.57659979606518),\n ('be', 104.11856816919685),\n ('thi', 103.06110204410142),\n ('on', 94.5230013722625),\n ('have', 89.88433983404117),\n ('with', 75.18149596453108),\n ('are', 74.93802609781713),\n ('if', 74.24248878944846),\n ('or', 74.23062669897583),\n ('do', 71.9015042136159),\n ('use', 71.4761519919797)]"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "dtm = vectorizer.fit_transform(train_tokenized)\n",
    "tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_column_stem = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_train_stem['TF', 'Без стоп-слов'] = tf_column_stem\n",
    "tf_column_stem"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:02.932388900Z",
     "start_time": "2023-12-17T11:11:02.701120100Z"
    }
   },
   "id": "5e6f7dc134877741"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью TfidfTransformer c стоп-словами c приминением стемминга TF"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2518131c743585d1"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "[('thi', 169.76884542624322),\n ('use', 116.9587739384261),\n ('ani', 75.52328947198923),\n ('key', 61.252598858687456),\n ('wa', 61.24004770417701),\n ('know', 58.981341558737796),\n ('doe', 57.240041179177425),\n ('like', 57.137968393386735),\n ('ha', 56.59160730937827),\n ('chip', 44.84136837996277),\n ('just', 44.83029998661235),\n ('thank', 41.73801066350643),\n ('work', 41.61901207046525),\n ('anyon', 41.18249764862851),\n ('look', 40.281971512326024),\n ('file', 38.83230683543829),\n ('need', 38.34165975253376),\n ('encrypt', 36.15778681744015),\n ('onli', 34.91770247819603),\n ('program', 33.24366815109684)]"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(train_tokenized)\n",
    "tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_column_stem_stop = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_train_stem['TF', 'С стоп-словами'] = tf_column_stem_stop\n",
    "tf_column_stem_stop"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:03.146252800Z",
     "start_time": "2023-12-17T11:11:02.932388900Z"
    }
   },
   "id": "c994c05b5a1f8758"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью TfidfTransformer без стоп-слов c приминением стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ca589e9e50f780b"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 214.47110783764484),\n ('to', 128.91799924792693),\n ('of', 101.9397995673158),\n ('and', 91.85258731878157),\n ('is', 84.95276457618122),\n ('it', 82.95304153471035),\n ('in', 73.08110129504044),\n ('that', 73.01505717147099),\n ('for', 68.08421571530532),\n ('you', 66.66827307818957),\n ('be', 59.109398696493265),\n ('thi', 55.45567314375222),\n ('on', 50.853304634787605),\n ('have', 50.30994107364992),\n ('are', 45.04857754230604),\n ('with', 44.02554259499939),\n ('if', 43.91731908930183),\n ('or', 43.60061441479198),\n ('do', 43.39462930056318),\n ('use', 42.979583779815044)]"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "dtm = vectorizer.fit_transform(train_tokenized)\n",
    "tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_idf_column = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_train_stem['TF-IDF', 'Без стоп-слов'] = tf_idf_column\n",
    "tf_idf_column"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:03.372248200Z",
     "start_time": "2023-12-17T11:11:03.148540400Z"
    }
   },
   "id": "c3c5f382c3169075"
  },
  {
   "cell_type": "markdown",
   "source": [
    "train tf-idf stemming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b0033bf826b4c7"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 214.47110783764484),\n ('to', 128.91799924792693),\n ('of', 101.9397995673158),\n ('and', 91.85258731878157),\n ('is', 84.95276457618122),\n ('it', 82.95304153471035),\n ('in', 73.08110129504044),\n ('that', 73.01505717147099),\n ('for', 68.08421571530532),\n ('you', 66.66827307818957),\n ('be', 59.109398696493265),\n ('thi', 55.45567314375222),\n ('on', 50.853304634787605),\n ('have', 50.30994107364992),\n ('are', 45.04857754230604),\n ('with', 44.02554259499939),\n ('if', 43.91731908930183),\n ('or', 43.60061441479198),\n ('do', 43.39462930056318),\n ('use', 42.979583779815044)]"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "dtm = vectorizer.fit_transform(train_tokenized)\n",
    "tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_idf_column_stop = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_train_stem['TF-IDF', 'С стоп-словами'] = tf_idf_column_stop\n",
    "tf_idf_column_stop"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:03.594303800Z",
     "start_time": "2023-12-17T11:11:03.372248200Z"
    }
   },
   "id": "6b992ebb1d1e8d46"
  },
  {
   "cell_type": "markdown",
   "source": [
    "test count steming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d633a291c03e5738"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 9063),\n ('to', 5360),\n ('of', 4137),\n ('and', 4073),\n ('is', 3139),\n ('in', 2612),\n ('it', 2562),\n ('for', 2362),\n ('that', 2237),\n ('you', 2086),\n ('be', 1647),\n ('thi', 1472),\n ('on', 1469),\n ('have', 1298),\n ('or', 1295),\n ('with', 1260),\n ('are', 1212),\n ('if', 1154),\n ('use', 1097),\n ('not', 1077)]"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000)\n",
    "dtm = vect.fit_transform(test_tokenized)\n",
    "count_column_stem_test = get_20_freq_words(vect, dtm)\n",
    "df_test_stem['Count', 'Без стоп-слов'] = count_column_stem_test\n",
    "count_column_stem_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:03.703697700Z",
     "start_time": "2023-12-17T11:11:03.594303800Z"
    }
   },
   "id": "39879f0d13bfa57d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "test count steming stop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1071eb3d3f725140"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "[('thi', 1472),\n ('use', 1097),\n ('imag', 998),\n ('file', 615),\n ('jpeg', 531),\n ('wa', 510),\n ('ani', 505),\n ('program', 497),\n ('ha', 479),\n ('edu', 468),\n ('like', 457),\n ('bit', 451),\n ('format', 411),\n ('know', 401),\n ('doe', 386),\n ('data', 369),\n ('onli', 344),\n ('work', 344),\n ('make', 341),\n ('just', 339)]"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vect.fit_transform(test_tokenized)\n",
    "count_column_stem_stop_test = get_20_freq_words(vect, dtm)\n",
    "df_test_stem['Count', 'С стоп-словами'] = count_column_stem_stop_test\n",
    "count_column_stem_stop_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:04.845477600Z",
     "start_time": "2023-12-17T11:11:03.703697700Z"
    }
   },
   "id": "c1a3ff5afab1c17e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "test tf steming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5668367ec6dd1bd8"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 344.5586615001451),\n ('to', 211.00830323462205),\n ('of', 150.4948643044924),\n ('and', 137.86770734884337),\n ('is', 121.90454220930677),\n ('it', 114.00279976905662),\n ('in', 102.13673946564619),\n ('that', 96.8675995748726),\n ('for', 90.52244749715135),\n ('you', 79.67798552554372),\n ('be', 68.85460938269497),\n ('on', 61.11286152167876),\n ('have', 60.90208541002496),\n ('thi', 60.34198459030236),\n ('or', 50.6519636067793),\n ('if', 49.388084620385364),\n ('with', 47.226534170832274),\n ('can', 46.08463643662672),\n ('are', 45.258192235058026),\n ('do', 44.57070053438678)]"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "dtm = vectorizer.fit_transform(test_tokenized)\n",
    "tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_column_stem_test = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_test_stem['TF', 'Без стоп-слов'] = tf_column_stem_test\n",
    "tf_column_stem_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:04.845477600Z",
     "start_time": "2023-12-17T11:11:03.822977600Z"
    }
   },
   "id": "bbce0042aa22074d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "test tf steming stop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f456dffabf73f1a"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "data": {
      "text/plain": "[('thi', 97.49594840488756),\n ('use', 71.55533662918268),\n ('ani', 44.25521347634618),\n ('know', 43.06179116896538),\n ('wa', 39.53046546637826),\n ('like', 36.7672996472524),\n ('ha', 34.4237897180173),\n ('doe', 34.19333412527219),\n ('just', 29.513176624612083),\n ('thank', 28.549402258522097),\n ('anyon', 28.298548977191338),\n ('work', 26.494787158165767),\n ('think', 24.640467411932118),\n ('need', 24.56412549445321),\n ('program', 24.56261574830257),\n ('look', 24.441912389427163),\n ('make', 24.08146082002033),\n ('key', 23.553337023165067),\n ('pleas', 23.374159220189213),\n ('onli', 22.125484501080784)]"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(test_tokenized)\n",
    "tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_column_stem_stop_test = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_test_stem['TF', 'С стоп-словами'] = tf_column_stem_stop_test\n",
    "tf_column_stem_stop_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:04.845477600Z",
     "start_time": "2023-12-17T11:11:03.974200300Z"
    }
   },
   "id": "d9dabbfac178beff"
  },
  {
   "cell_type": "markdown",
   "source": [
    "test tf-idf steming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "230b953b48154652"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 133.4538752492904),\n ('to', 85.99632390188842),\n ('of', 65.18554426844001),\n ('and', 59.63534071325143),\n ('it', 55.94816487314925),\n ('is', 55.94487331494873),\n ('that', 48.31727363290116),\n ('in', 47.30170083345097),\n ('you', 45.68450447600251),\n ('for', 43.47570804261264),\n ('be', 37.344622742852174),\n ('thi', 34.3491285954964),\n ('have', 34.04975283966364),\n ('on', 33.209071780256735),\n ('if', 29.438597446898267),\n ('or', 29.11817727731375),\n ('can', 28.41866932660334),\n ('are', 28.194428691654128),\n ('do', 27.903939642519983),\n ('not', 27.900212314318484)]"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "dtm = vectorizer.fit_transform(test_tokenized)\n",
    "tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_idf_column_stem_test = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_test_stem['TF-IDF', 'Без стоп-слов'] = tf_idf_column_stem_test\n",
    "tf_idf_column_stem_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:04.845477600Z",
     "start_time": "2023-12-17T11:11:04.120739700Z"
    }
   },
   "id": "9d29c5a1180aada0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "test tf-idf steming stop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e11b8f3b6c2e9ede"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "[('thi', 97.49594840488756),\n ('use', 71.55533662918268),\n ('ani', 44.25521347634618),\n ('know', 43.06179116896538),\n ('wa', 39.53046546637826),\n ('like', 36.7672996472524),\n ('ha', 34.4237897180173),\n ('doe', 34.19333412527219),\n ('just', 29.513176624612083),\n ('thank', 28.549402258522097),\n ('anyon', 28.298548977191338),\n ('work', 26.494787158165767),\n ('think', 24.640467411932118),\n ('need', 24.56412549445321),\n ('program', 24.56261574830257),\n ('look', 24.441912389427163),\n ('make', 24.08146082002033),\n ('key', 23.553337023165067),\n ('pleas', 23.374159220189213),\n ('onli', 22.125484501080784)]"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(test_tokenized)\n",
    "tfidf = TfidfTransformer(use_idf=False).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "tf_idf_column_stem_stop_test = get_20_freq_words_idf(feature_names, tfidf_values)\n",
    "df_test_stem['TF-IDF', 'С стоп-словами'] = tf_idf_column_stem_stop_test\n",
    "tf_idf_column_stem_stop_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:04.845477600Z",
     "start_time": "2023-12-17T11:11:04.266668700Z"
    }
   },
   "id": "bf58c378620d26b"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "           Count                                              TF  \\\n   Без стоп-слов      С стоп-словами               Без стоп-слов   \n0   (the, 16689)          (key, 937)    (the, 573.5878746017531)   \n1     (to, 8883)          (use, 932)    (to, 323.08392380711285)   \n2     (of, 7021)         (like, 642)    (of, 242.72988399270668)   \n3    (and, 6843)          (don, 592)   (and, 217.11039200783313)   \n4     (is, 5467)           (db, 562)    (is, 186.06314755308978)   \n5     (in, 4416)          (edu, 553)    (it, 169.68759255688352)   \n6     (it, 3900)   (encryption, 552)     (in, 162.3235233209576)   \n7   (that, 3682)         (data, 547)   (for, 148.54566211510095)   \n8    (for, 3677)         (know, 542)  (that, 146.62595086524686)   \n9    (you, 2852)         (just, 533)   (you, 120.10726765931362)   \n10    (be, 2788)         (chip, 521)  (this, 105.17732299778746)   \n11  (this, 2585)         (does, 501)     (be, 98.40643227700879)   \n12    (on, 2451)         (used, 498)     (on, 96.18830762929639)   \n13   (are, 2155)  (information, 497)   (have, 86.45612956176703)   \n14  (with, 2111)        (image, 492)   (with, 76.85987967685325)   \n15    (or, 2090)       (people, 483)     (if, 75.80342481606581)   \n16  (have, 1879)         (time, 447)     (or, 75.78439948121508)   \n17    (as, 1784)          (bit, 437)    (are, 74.94851602350174)   \n18   (can, 1704)         (file, 427)    (can, 68.77692347543262)   \n19    (if, 1702)     (graphics, 423)     (not, 63.1163398649122)   \n\n                                                          TF-IDF  \\\n                      С стоп-словами               Без стоп-слов   \n0          (use, 58.433522994302784)   (the, 214.27923011983643)   \n1         (know, 57.307906073228544)    (to, 128.50658234239415)   \n2          (like, 56.64353296005652)    (of, 101.86203264775209)   \n3          (don, 49.839861337882176)      (and, 91.733961272254)   \n4         (just, 49.002983276208454)     (is, 83.97727336110019)   \n5           (does, 48.9585366753105)     (it, 79.47554322700573)   \n6             (key, 48.142224601803)     (in, 72.70953833589493)   \n7        (thanks, 39.15072837325299)   (that, 72.51290798766651)   \n8          (chip, 35.95980854400468)    (for, 67.66475343892618)   \n9          (good, 35.08471270781613)    (you, 66.41180952122423)   \n10         (need, 32.70371344296186)     (be, 55.65313856163747)   \n11         (used, 31.85354729678519)   (this, 55.13173979874598)   \n12       (think, 31.408220640281492)     (on, 50.56554310512375)   \n13          (ve, 31.265005741849148)   (have, 48.11707358740201)   \n14        (time, 30.354985689209872)    (are, 44.12242787539058)   \n15      (people, 30.323719447911316)  (with, 43.883460758799835)   \n16  (encryption, 28.319287928894738)     (if, 43.62474130570933)   \n17        (using, 27.62980799728084)     (or, 43.39770046917348)   \n18    (graphics, 27.205639758499185)    (can, 40.89942210826312)   \n19     (clipper, 26.571725044843287)    (as, 40.345366794020045)   \n\n                                      \n                      С стоп-словами  \n0          (key, 31.639250020331772)  \n1         (know, 29.275985507770525)  \n2            (use, 28.5909199903978)  \n3          (like, 27.36974112627459)  \n4          (does, 27.32020401912984)  \n5          (don, 25.935560274661245)  \n6         (just, 25.519485103509275)  \n7         (chip, 24.453605471872685)  \n8        (thanks, 24.11088770306822)  \n9   (encryption, 20.612197002323146)  \n10        (good, 20.345796034175873)  \n11        (need, 19.637524126929815)  \n12          (ve, 19.573641488242785)  \n13    (graphics, 19.457067951393185)  \n14     (clipper, 19.278143372470023)  \n15      (people, 18.825771055423083)  \n16       (think, 18.438571829249657)  \n17        (used, 18.158442956203725)  \n18  (government, 17.929344754138906)  \n19          (time, 17.5444284746565)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"2\" halign=\"left\">Count</th>\n      <th colspan=\"2\" halign=\"left\">TF</th>\n      <th colspan=\"2\" halign=\"left\">TF-IDF</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(the, 16689)</td>\n      <td>(key, 937)</td>\n      <td>(the, 573.5878746017531)</td>\n      <td>(use, 58.433522994302784)</td>\n      <td>(the, 214.27923011983643)</td>\n      <td>(key, 31.639250020331772)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(to, 8883)</td>\n      <td>(use, 932)</td>\n      <td>(to, 323.08392380711285)</td>\n      <td>(know, 57.307906073228544)</td>\n      <td>(to, 128.50658234239415)</td>\n      <td>(know, 29.275985507770525)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(of, 7021)</td>\n      <td>(like, 642)</td>\n      <td>(of, 242.72988399270668)</td>\n      <td>(like, 56.64353296005652)</td>\n      <td>(of, 101.86203264775209)</td>\n      <td>(use, 28.5909199903978)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(and, 6843)</td>\n      <td>(don, 592)</td>\n      <td>(and, 217.11039200783313)</td>\n      <td>(don, 49.839861337882176)</td>\n      <td>(and, 91.733961272254)</td>\n      <td>(like, 27.36974112627459)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(is, 5467)</td>\n      <td>(db, 562)</td>\n      <td>(is, 186.06314755308978)</td>\n      <td>(just, 49.002983276208454)</td>\n      <td>(is, 83.97727336110019)</td>\n      <td>(does, 27.32020401912984)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(in, 4416)</td>\n      <td>(edu, 553)</td>\n      <td>(it, 169.68759255688352)</td>\n      <td>(does, 48.9585366753105)</td>\n      <td>(it, 79.47554322700573)</td>\n      <td>(don, 25.935560274661245)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>(it, 3900)</td>\n      <td>(encryption, 552)</td>\n      <td>(in, 162.3235233209576)</td>\n      <td>(key, 48.142224601803)</td>\n      <td>(in, 72.70953833589493)</td>\n      <td>(just, 25.519485103509275)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>(that, 3682)</td>\n      <td>(data, 547)</td>\n      <td>(for, 148.54566211510095)</td>\n      <td>(thanks, 39.15072837325299)</td>\n      <td>(that, 72.51290798766651)</td>\n      <td>(chip, 24.453605471872685)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(for, 3677)</td>\n      <td>(know, 542)</td>\n      <td>(that, 146.62595086524686)</td>\n      <td>(chip, 35.95980854400468)</td>\n      <td>(for, 67.66475343892618)</td>\n      <td>(thanks, 24.11088770306822)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>(you, 2852)</td>\n      <td>(just, 533)</td>\n      <td>(you, 120.10726765931362)</td>\n      <td>(good, 35.08471270781613)</td>\n      <td>(you, 66.41180952122423)</td>\n      <td>(encryption, 20.612197002323146)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>(be, 2788)</td>\n      <td>(chip, 521)</td>\n      <td>(this, 105.17732299778746)</td>\n      <td>(need, 32.70371344296186)</td>\n      <td>(be, 55.65313856163747)</td>\n      <td>(good, 20.345796034175873)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>(this, 2585)</td>\n      <td>(does, 501)</td>\n      <td>(be, 98.40643227700879)</td>\n      <td>(used, 31.85354729678519)</td>\n      <td>(this, 55.13173979874598)</td>\n      <td>(need, 19.637524126929815)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(on, 2451)</td>\n      <td>(used, 498)</td>\n      <td>(on, 96.18830762929639)</td>\n      <td>(think, 31.408220640281492)</td>\n      <td>(on, 50.56554310512375)</td>\n      <td>(ve, 19.573641488242785)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>(are, 2155)</td>\n      <td>(information, 497)</td>\n      <td>(have, 86.45612956176703)</td>\n      <td>(ve, 31.265005741849148)</td>\n      <td>(have, 48.11707358740201)</td>\n      <td>(graphics, 19.457067951393185)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>(with, 2111)</td>\n      <td>(image, 492)</td>\n      <td>(with, 76.85987967685325)</td>\n      <td>(time, 30.354985689209872)</td>\n      <td>(are, 44.12242787539058)</td>\n      <td>(clipper, 19.278143372470023)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>(or, 2090)</td>\n      <td>(people, 483)</td>\n      <td>(if, 75.80342481606581)</td>\n      <td>(people, 30.323719447911316)</td>\n      <td>(with, 43.883460758799835)</td>\n      <td>(people, 18.825771055423083)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>(have, 1879)</td>\n      <td>(time, 447)</td>\n      <td>(or, 75.78439948121508)</td>\n      <td>(encryption, 28.319287928894738)</td>\n      <td>(if, 43.62474130570933)</td>\n      <td>(think, 18.438571829249657)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>(as, 1784)</td>\n      <td>(bit, 437)</td>\n      <td>(are, 74.94851602350174)</td>\n      <td>(using, 27.62980799728084)</td>\n      <td>(or, 43.39770046917348)</td>\n      <td>(used, 18.158442956203725)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>(can, 1704)</td>\n      <td>(file, 427)</td>\n      <td>(can, 68.77692347543262)</td>\n      <td>(graphics, 27.205639758499185)</td>\n      <td>(can, 40.89942210826312)</td>\n      <td>(government, 17.929344754138906)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>(if, 1702)</td>\n      <td>(graphics, 423)</td>\n      <td>(not, 63.1163398649122)</td>\n      <td>(clipper, 26.571725044843287)</td>\n      <td>(as, 40.345366794020045)</td>\n      <td>(time, 17.5444284746565)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:04.845477600Z",
     "start_time": "2023-12-17T11:11:04.437748900Z"
    }
   },
   "id": "cacfa26199053873"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "           Count                                           TF  \\\n   Без стоп-слов    С стоп-словами              Без стоп-слов   \n0    (the, 9066)      (image, 666)   (the, 351.8833889205307)   \n1     (to, 5360)       (jpeg, 526)   (to, 215.21048224463186)   \n2     (of, 4137)        (use, 516)   (of, 153.66848738794562)   \n3    (and, 4073)        (edu, 468)  (and, 140.95239513687198)   \n4     (is, 3074)   (graphics, 462)     (is, 120.607868681005)   \n5     (in, 2610)       (like, 408)      (it, 110.88904071825)   \n6     (it, 2402)       (file, 389)   (in, 104.33536769410595)   \n7    (for, 2362)        (don, 378)  (that, 98.57686193779719)   \n8   (that, 2228)       (data, 368)   (for, 92.40323586413749)   \n9    (you, 2086)       (know, 355)    (you, 81.3739413442512)   \n10    (be, 1535)       (just, 339)      (be, 65.317483332802)   \n11  (this, 1472)        (bit, 337)    (on, 62.06126483104049)   \n12    (on, 1462)  (available, 325)  (this, 61.59725664266683)   \n13    (or, 1295)   (software, 324)  (have, 57.61931147617616)   \n14  (with, 1258)     (images, 307)   (or, 51.777520899187046)   \n15  (have, 1215)    (program, 298)    (if, 50.32542218561264)   \n16   (are, 1186)       (does, 291)   (can, 49.21418012871439)   \n17    (if, 1154)       (time, 282)  (with, 48.18761054093589)   \n18   (can, 1101)       (used, 272)   (are, 45.11043625995436)   \n19    (as, 1026)        (ftp, 271)   (not, 43.53696710250099)   \n\n                                                          TF-IDF  \\\n                      С стоп-словами               Без стоп-слов   \n0          (know, 42.06416144310403)   (the, 132.73987348944183)   \n1         (like, 36.822127481402006)     (to, 85.24139589140572)   \n2          (use, 36.792352685034714)      (of, 64.8113206497389)   \n3          (just, 32.40544450511485)   (and, 59.482599607653924)   \n4          (don, 30.469003399281732)     (is, 54.52831275498656)   \n5         (does, 29.885181249520144)     (it, 53.75496178917013)   \n6        (thanks, 27.29259689277586)  (that, 47.770429081592496)   \n7         (think, 24.82104074312471)     (in, 47.06683267580104)   \n8         (used, 21.459669060506144)    (you, 45.42774190604445)   \n9         (need, 20.686196868337543)   (for, 43.184971775965785)   \n10    (graphics, 20.679160827517396)     (be, 35.43112523777322)   \n11        (time, 20.129142247758516)   (this, 34.08603226381701)   \n12     (program, 19.584807797820034)     (on, 32.90027716297289)   \n13      (people, 19.102579785729354)  (have, 32.025491313098804)   \n14         (chip, 18.54157887888643)     (if, 29.12956299199104)   \n15          (edu, 18.28349107306669)    (or, 29.011357783500337)   \n16          (ve, 18.270863639958304)    (can, 28.82297538311695)   \n17  (government, 18.089022419581863)    (are, 27.38799920531715)   \n18        (good, 17.967957804096248)   (with, 27.03257285835745)   \n19         (bit, 17.440629791897027)     (not, 26.9403506133788)   \n\n                                      \n                      С стоп-словами  \n0         (know, 21.435150574064025)  \n1         (like, 18.950713675342715)  \n2          (use, 18.420212557191185)  \n3       (thanks, 17.098531494337998)  \n4          (does, 16.98361584501106)  \n5         (just, 16.684961307647363)  \n6          (don, 16.428887334276713)  \n7        (think, 14.557804515797576)  \n8     (graphics, 14.252209873941238)  \n9       (program, 13.64987318610155)  \n10  (government, 12.964530900368489)  \n11        (chip, 12.860296724296857)  \n12        (used, 12.439336643011975)  \n13      (people, 12.333813168768687)  \n14        (need, 12.240898928564446)  \n15         (bit, 12.015183440146776)  \n16         (edu, 11.906194792288247)  \n17          (ve, 11.870735103204861)  \n18        (time, 11.745018935759806)  \n19         (key, 11.670618169470233)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"2\" halign=\"left\">Count</th>\n      <th colspan=\"2\" halign=\"left\">TF</th>\n      <th colspan=\"2\" halign=\"left\">TF-IDF</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(the, 9066)</td>\n      <td>(image, 666)</td>\n      <td>(the, 351.8833889205307)</td>\n      <td>(know, 42.06416144310403)</td>\n      <td>(the, 132.73987348944183)</td>\n      <td>(know, 21.435150574064025)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(to, 5360)</td>\n      <td>(jpeg, 526)</td>\n      <td>(to, 215.21048224463186)</td>\n      <td>(like, 36.822127481402006)</td>\n      <td>(to, 85.24139589140572)</td>\n      <td>(like, 18.950713675342715)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(of, 4137)</td>\n      <td>(use, 516)</td>\n      <td>(of, 153.66848738794562)</td>\n      <td>(use, 36.792352685034714)</td>\n      <td>(of, 64.8113206497389)</td>\n      <td>(use, 18.420212557191185)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(and, 4073)</td>\n      <td>(edu, 468)</td>\n      <td>(and, 140.95239513687198)</td>\n      <td>(just, 32.40544450511485)</td>\n      <td>(and, 59.482599607653924)</td>\n      <td>(thanks, 17.098531494337998)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(is, 3074)</td>\n      <td>(graphics, 462)</td>\n      <td>(is, 120.607868681005)</td>\n      <td>(don, 30.469003399281732)</td>\n      <td>(is, 54.52831275498656)</td>\n      <td>(does, 16.98361584501106)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(in, 2610)</td>\n      <td>(like, 408)</td>\n      <td>(it, 110.88904071825)</td>\n      <td>(does, 29.885181249520144)</td>\n      <td>(it, 53.75496178917013)</td>\n      <td>(just, 16.684961307647363)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>(it, 2402)</td>\n      <td>(file, 389)</td>\n      <td>(in, 104.33536769410595)</td>\n      <td>(thanks, 27.29259689277586)</td>\n      <td>(that, 47.770429081592496)</td>\n      <td>(don, 16.428887334276713)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>(for, 2362)</td>\n      <td>(don, 378)</td>\n      <td>(that, 98.57686193779719)</td>\n      <td>(think, 24.82104074312471)</td>\n      <td>(in, 47.06683267580104)</td>\n      <td>(think, 14.557804515797576)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(that, 2228)</td>\n      <td>(data, 368)</td>\n      <td>(for, 92.40323586413749)</td>\n      <td>(used, 21.459669060506144)</td>\n      <td>(you, 45.42774190604445)</td>\n      <td>(graphics, 14.252209873941238)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>(you, 2086)</td>\n      <td>(know, 355)</td>\n      <td>(you, 81.3739413442512)</td>\n      <td>(need, 20.686196868337543)</td>\n      <td>(for, 43.184971775965785)</td>\n      <td>(program, 13.64987318610155)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>(be, 1535)</td>\n      <td>(just, 339)</td>\n      <td>(be, 65.317483332802)</td>\n      <td>(graphics, 20.679160827517396)</td>\n      <td>(be, 35.43112523777322)</td>\n      <td>(government, 12.964530900368489)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>(this, 1472)</td>\n      <td>(bit, 337)</td>\n      <td>(on, 62.06126483104049)</td>\n      <td>(time, 20.129142247758516)</td>\n      <td>(this, 34.08603226381701)</td>\n      <td>(chip, 12.860296724296857)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(on, 1462)</td>\n      <td>(available, 325)</td>\n      <td>(this, 61.59725664266683)</td>\n      <td>(program, 19.584807797820034)</td>\n      <td>(on, 32.90027716297289)</td>\n      <td>(used, 12.439336643011975)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>(or, 1295)</td>\n      <td>(software, 324)</td>\n      <td>(have, 57.61931147617616)</td>\n      <td>(people, 19.102579785729354)</td>\n      <td>(have, 32.025491313098804)</td>\n      <td>(people, 12.333813168768687)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>(with, 1258)</td>\n      <td>(images, 307)</td>\n      <td>(or, 51.777520899187046)</td>\n      <td>(chip, 18.54157887888643)</td>\n      <td>(if, 29.12956299199104)</td>\n      <td>(need, 12.240898928564446)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>(have, 1215)</td>\n      <td>(program, 298)</td>\n      <td>(if, 50.32542218561264)</td>\n      <td>(edu, 18.28349107306669)</td>\n      <td>(or, 29.011357783500337)</td>\n      <td>(bit, 12.015183440146776)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>(are, 1186)</td>\n      <td>(does, 291)</td>\n      <td>(can, 49.21418012871439)</td>\n      <td>(ve, 18.270863639958304)</td>\n      <td>(can, 28.82297538311695)</td>\n      <td>(edu, 11.906194792288247)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>(if, 1154)</td>\n      <td>(time, 282)</td>\n      <td>(with, 48.18761054093589)</td>\n      <td>(government, 18.089022419581863)</td>\n      <td>(are, 27.38799920531715)</td>\n      <td>(ve, 11.870735103204861)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>(can, 1101)</td>\n      <td>(used, 272)</td>\n      <td>(are, 45.11043625995436)</td>\n      <td>(good, 17.967957804096248)</td>\n      <td>(with, 27.03257285835745)</td>\n      <td>(time, 11.745018935759806)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>(as, 1026)</td>\n      <td>(ftp, 271)</td>\n      <td>(not, 43.53696710250099)</td>\n      <td>(bit, 17.440629791897027)</td>\n      <td>(not, 26.9403506133788)</td>\n      <td>(key, 11.670618169470233)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:04.845477600Z",
     "start_time": "2023-12-17T11:11:04.458739700Z"
    }
   },
   "id": "4155b683fb7bf197"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "           Count                                          TF  \\\n   Без стоп-слов  С стоп-словами               Без стоп-слов   \n0   (the, 16688)     (thi, 2585)    (the, 561.4880559369901)   \n1     (to, 8883)     (use, 2014)     (to, 316.3652287700625)   \n2     (of, 7021)     (key, 1283)    (of, 237.41198848231448)   \n3    (and, 6843)       (ha, 887)   (and, 212.34202585341927)   \n4     (is, 5549)      (ani, 866)    (is, 185.57446689326932)   \n5     (in, 4419)       (wa, 783)    (it, 175.55160439683934)   \n6     (it, 4191)  (encrypt, 774)    (in, 159.25693721881674)   \n7   (that, 3692)     (imag, 737)   (for, 145.53831889691298)   \n8    (for, 3677)     (file, 730)  (that, 144.01545732916136)   \n9     (be, 2998)     (like, 711)   (you, 117.57659979606518)   \n10   (you, 2852)     (chip, 672)    (be, 104.11856816919685)   \n11   (thi, 2585)      (doe, 671)   (thi, 103.06110204410142)   \n12    (on, 2459)     (know, 622)      (on, 94.5230013722625)   \n13   (are, 2195)      (bit, 621)   (have, 89.88433983404117)   \n14  (with, 2111)  (program, 569)   (with, 75.18149596453108)   \n15    (or, 2090)       (db, 562)    (are, 74.93802609781713)   \n16   (use, 2014)     (onli, 560)     (if, 74.24248878944846)   \n17  (have, 1997)      (edu, 553)     (or, 74.23062669897583)   \n18    (as, 1784)     (data, 548)      (do, 71.9015042136159)   \n19   (not, 1740)    (secur, 534)     (use, 71.4761519919797)   \n\n                                                     TF-IDF  \\\n                  С стоп-словами              Без стоп-слов   \n0      (thi, 169.76884542624322)  (the, 214.47110783764484)   \n1       (use, 116.9587739384261)   (to, 128.91799924792693)   \n2       (ani, 75.52328947198923)    (of, 101.9397995673158)   \n3      (key, 61.252598858687456)   (and, 91.85258731878157)   \n4        (wa, 61.24004770417701)    (is, 84.95276457618122)   \n5     (know, 58.981341558737796)    (it, 82.95304153471035)   \n6      (doe, 57.240041179177425)    (in, 73.08110129504044)   \n7     (like, 57.137968393386735)  (that, 73.01505717147099)   \n8        (ha, 56.59160730937827)   (for, 68.08421571530532)   \n9      (chip, 44.84136837996277)   (you, 66.66827307818957)   \n10     (just, 44.83029998661235)   (be, 59.109398696493265)   \n11    (thank, 41.73801066350643)   (thi, 55.45567314375222)   \n12     (work, 41.61901207046525)   (on, 50.853304634787605)   \n13    (anyon, 41.18249764862851)  (have, 50.30994107364992)   \n14    (look, 40.281971512326024)   (are, 45.04857754230604)   \n15     (file, 38.83230683543829)  (with, 44.02554259499939)   \n16     (need, 38.34165975253376)    (if, 43.91731908930183)   \n17  (encrypt, 36.15778681744015)    (or, 43.60061441479198)   \n18     (onli, 34.91770247819603)    (do, 43.39462930056318)   \n19  (program, 33.24366815109684)  (use, 42.979583779815044)   \n\n                               \n               С стоп-словами  \n0   (the, 214.47110783764484)  \n1    (to, 128.91799924792693)  \n2     (of, 101.9397995673158)  \n3    (and, 91.85258731878157)  \n4     (is, 84.95276457618122)  \n5     (it, 82.95304153471035)  \n6     (in, 73.08110129504044)  \n7   (that, 73.01505717147099)  \n8    (for, 68.08421571530532)  \n9    (you, 66.66827307818957)  \n10   (be, 59.109398696493265)  \n11   (thi, 55.45567314375222)  \n12   (on, 50.853304634787605)  \n13  (have, 50.30994107364992)  \n14   (are, 45.04857754230604)  \n15  (with, 44.02554259499939)  \n16    (if, 43.91731908930183)  \n17    (or, 43.60061441479198)  \n18    (do, 43.39462930056318)  \n19  (use, 42.979583779815044)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"2\" halign=\"left\">Count</th>\n      <th colspan=\"2\" halign=\"left\">TF</th>\n      <th colspan=\"2\" halign=\"left\">TF-IDF</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(the, 16688)</td>\n      <td>(thi, 2585)</td>\n      <td>(the, 561.4880559369901)</td>\n      <td>(thi, 169.76884542624322)</td>\n      <td>(the, 214.47110783764484)</td>\n      <td>(the, 214.47110783764484)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(to, 8883)</td>\n      <td>(use, 2014)</td>\n      <td>(to, 316.3652287700625)</td>\n      <td>(use, 116.9587739384261)</td>\n      <td>(to, 128.91799924792693)</td>\n      <td>(to, 128.91799924792693)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(of, 7021)</td>\n      <td>(key, 1283)</td>\n      <td>(of, 237.41198848231448)</td>\n      <td>(ani, 75.52328947198923)</td>\n      <td>(of, 101.9397995673158)</td>\n      <td>(of, 101.9397995673158)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(and, 6843)</td>\n      <td>(ha, 887)</td>\n      <td>(and, 212.34202585341927)</td>\n      <td>(key, 61.252598858687456)</td>\n      <td>(and, 91.85258731878157)</td>\n      <td>(and, 91.85258731878157)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(is, 5549)</td>\n      <td>(ani, 866)</td>\n      <td>(is, 185.57446689326932)</td>\n      <td>(wa, 61.24004770417701)</td>\n      <td>(is, 84.95276457618122)</td>\n      <td>(is, 84.95276457618122)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(in, 4419)</td>\n      <td>(wa, 783)</td>\n      <td>(it, 175.55160439683934)</td>\n      <td>(know, 58.981341558737796)</td>\n      <td>(it, 82.95304153471035)</td>\n      <td>(it, 82.95304153471035)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>(it, 4191)</td>\n      <td>(encrypt, 774)</td>\n      <td>(in, 159.25693721881674)</td>\n      <td>(doe, 57.240041179177425)</td>\n      <td>(in, 73.08110129504044)</td>\n      <td>(in, 73.08110129504044)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>(that, 3692)</td>\n      <td>(imag, 737)</td>\n      <td>(for, 145.53831889691298)</td>\n      <td>(like, 57.137968393386735)</td>\n      <td>(that, 73.01505717147099)</td>\n      <td>(that, 73.01505717147099)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(for, 3677)</td>\n      <td>(file, 730)</td>\n      <td>(that, 144.01545732916136)</td>\n      <td>(ha, 56.59160730937827)</td>\n      <td>(for, 68.08421571530532)</td>\n      <td>(for, 68.08421571530532)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>(be, 2998)</td>\n      <td>(like, 711)</td>\n      <td>(you, 117.57659979606518)</td>\n      <td>(chip, 44.84136837996277)</td>\n      <td>(you, 66.66827307818957)</td>\n      <td>(you, 66.66827307818957)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>(you, 2852)</td>\n      <td>(chip, 672)</td>\n      <td>(be, 104.11856816919685)</td>\n      <td>(just, 44.83029998661235)</td>\n      <td>(be, 59.109398696493265)</td>\n      <td>(be, 59.109398696493265)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>(thi, 2585)</td>\n      <td>(doe, 671)</td>\n      <td>(thi, 103.06110204410142)</td>\n      <td>(thank, 41.73801066350643)</td>\n      <td>(thi, 55.45567314375222)</td>\n      <td>(thi, 55.45567314375222)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(on, 2459)</td>\n      <td>(know, 622)</td>\n      <td>(on, 94.5230013722625)</td>\n      <td>(work, 41.61901207046525)</td>\n      <td>(on, 50.853304634787605)</td>\n      <td>(on, 50.853304634787605)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>(are, 2195)</td>\n      <td>(bit, 621)</td>\n      <td>(have, 89.88433983404117)</td>\n      <td>(anyon, 41.18249764862851)</td>\n      <td>(have, 50.30994107364992)</td>\n      <td>(have, 50.30994107364992)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>(with, 2111)</td>\n      <td>(program, 569)</td>\n      <td>(with, 75.18149596453108)</td>\n      <td>(look, 40.281971512326024)</td>\n      <td>(are, 45.04857754230604)</td>\n      <td>(are, 45.04857754230604)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>(or, 2090)</td>\n      <td>(db, 562)</td>\n      <td>(are, 74.93802609781713)</td>\n      <td>(file, 38.83230683543829)</td>\n      <td>(with, 44.02554259499939)</td>\n      <td>(with, 44.02554259499939)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>(use, 2014)</td>\n      <td>(onli, 560)</td>\n      <td>(if, 74.24248878944846)</td>\n      <td>(need, 38.34165975253376)</td>\n      <td>(if, 43.91731908930183)</td>\n      <td>(if, 43.91731908930183)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>(have, 1997)</td>\n      <td>(edu, 553)</td>\n      <td>(or, 74.23062669897583)</td>\n      <td>(encrypt, 36.15778681744015)</td>\n      <td>(or, 43.60061441479198)</td>\n      <td>(or, 43.60061441479198)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>(as, 1784)</td>\n      <td>(data, 548)</td>\n      <td>(do, 71.9015042136159)</td>\n      <td>(onli, 34.91770247819603)</td>\n      <td>(do, 43.39462930056318)</td>\n      <td>(do, 43.39462930056318)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>(not, 1740)</td>\n      <td>(secur, 534)</td>\n      <td>(use, 71.4761519919797)</td>\n      <td>(program, 33.24366815109684)</td>\n      <td>(use, 42.979583779815044)</td>\n      <td>(use, 42.979583779815044)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_stem"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:04.864883300Z",
     "start_time": "2023-12-17T11:11:04.480042300Z"
    }
   },
   "id": "f1a0aab4e9fb17e0"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "           Count                                          TF  \\\n   Без стоп-слов  С стоп-словами               Без стоп-слов   \n0    (the, 9063)     (thi, 1472)    (the, 344.5586615001451)   \n1     (to, 5360)     (use, 1097)    (to, 211.00830323462205)   \n2     (of, 4137)     (imag, 998)     (of, 150.4948643044924)   \n3    (and, 4073)     (file, 615)   (and, 137.86770734884337)   \n4     (is, 3139)     (jpeg, 531)    (is, 121.90454220930677)   \n5     (in, 2612)       (wa, 510)    (it, 114.00279976905662)   \n6     (it, 2562)      (ani, 505)    (in, 102.13673946564619)   \n7    (for, 2362)  (program, 497)    (that, 96.8675995748726)   \n8   (that, 2237)       (ha, 479)    (for, 90.52244749715135)   \n9    (you, 2086)      (edu, 468)    (you, 79.67798552554372)   \n10    (be, 1647)     (like, 457)     (be, 68.85460938269497)   \n11   (thi, 1472)      (bit, 451)     (on, 61.11286152167876)   \n12    (on, 1469)   (format, 411)   (have, 60.90208541002496)   \n13  (have, 1298)     (know, 401)    (thi, 60.34198459030236)   \n14    (or, 1295)      (doe, 386)      (or, 50.6519636067793)   \n15  (with, 1260)     (data, 369)    (if, 49.388084620385364)   \n16   (are, 1212)     (onli, 344)  (with, 47.226534170832274)   \n17    (if, 1154)     (work, 344)    (can, 46.08463643662672)   \n18   (use, 1097)     (make, 341)   (are, 45.258192235058026)   \n19   (not, 1077)     (just, 339)     (do, 44.57070053438678)   \n\n                                                     TF-IDF  \\\n                  С стоп-словами              Без стоп-слов   \n0       (thi, 97.49594840488756)   (the, 133.4538752492904)   \n1       (use, 71.55533662918268)    (to, 85.99632390188842)   \n2       (ani, 44.25521347634618)    (of, 65.18554426844001)   \n3      (know, 43.06179116896538)   (and, 59.63534071325143)   \n4        (wa, 39.53046546637826)    (it, 55.94816487314925)   \n5       (like, 36.7672996472524)    (is, 55.94487331494873)   \n6         (ha, 34.4237897180173)  (that, 48.31727363290116)   \n7       (doe, 34.19333412527219)    (in, 47.30170083345097)   \n8     (just, 29.513176624612083)   (you, 45.68450447600251)   \n9    (thank, 28.549402258522097)   (for, 43.47570804261264)   \n10   (anyon, 28.298548977191338)   (be, 37.344622742852174)   \n11    (work, 26.494787158165767)    (thi, 34.3491285954964)   \n12   (think, 24.640467411932118)  (have, 34.04975283966364)   \n13     (need, 24.56412549445321)   (on, 33.209071780256735)   \n14  (program, 24.56261574830257)   (if, 29.438597446898267)   \n15    (look, 24.441912389427163)    (or, 29.11817727731375)   \n16     (make, 24.08146082002033)   (can, 28.41866932660334)   \n17     (key, 23.553337023165067)  (are, 28.194428691654128)   \n18   (pleas, 23.374159220189213)   (do, 27.903939642519983)   \n19    (onli, 22.125484501080784)  (not, 27.900212314318484)   \n\n                                  \n                  С стоп-словами  \n0       (thi, 97.49594840488756)  \n1       (use, 71.55533662918268)  \n2       (ani, 44.25521347634618)  \n3      (know, 43.06179116896538)  \n4        (wa, 39.53046546637826)  \n5       (like, 36.7672996472524)  \n6         (ha, 34.4237897180173)  \n7       (doe, 34.19333412527219)  \n8     (just, 29.513176624612083)  \n9    (thank, 28.549402258522097)  \n10   (anyon, 28.298548977191338)  \n11    (work, 26.494787158165767)  \n12   (think, 24.640467411932118)  \n13     (need, 24.56412549445321)  \n14  (program, 24.56261574830257)  \n15    (look, 24.441912389427163)  \n16     (make, 24.08146082002033)  \n17     (key, 23.553337023165067)  \n18   (pleas, 23.374159220189213)  \n19    (onli, 22.125484501080784)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"2\" halign=\"left\">Count</th>\n      <th colspan=\"2\" halign=\"left\">TF</th>\n      <th colspan=\"2\" halign=\"left\">TF-IDF</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n      <th>Без стоп-слов</th>\n      <th>С стоп-словами</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>(the, 9063)</td>\n      <td>(thi, 1472)</td>\n      <td>(the, 344.5586615001451)</td>\n      <td>(thi, 97.49594840488756)</td>\n      <td>(the, 133.4538752492904)</td>\n      <td>(thi, 97.49594840488756)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>(to, 5360)</td>\n      <td>(use, 1097)</td>\n      <td>(to, 211.00830323462205)</td>\n      <td>(use, 71.55533662918268)</td>\n      <td>(to, 85.99632390188842)</td>\n      <td>(use, 71.55533662918268)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>(of, 4137)</td>\n      <td>(imag, 998)</td>\n      <td>(of, 150.4948643044924)</td>\n      <td>(ani, 44.25521347634618)</td>\n      <td>(of, 65.18554426844001)</td>\n      <td>(ani, 44.25521347634618)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>(and, 4073)</td>\n      <td>(file, 615)</td>\n      <td>(and, 137.86770734884337)</td>\n      <td>(know, 43.06179116896538)</td>\n      <td>(and, 59.63534071325143)</td>\n      <td>(know, 43.06179116896538)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(is, 3139)</td>\n      <td>(jpeg, 531)</td>\n      <td>(is, 121.90454220930677)</td>\n      <td>(wa, 39.53046546637826)</td>\n      <td>(it, 55.94816487314925)</td>\n      <td>(wa, 39.53046546637826)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>(in, 2612)</td>\n      <td>(wa, 510)</td>\n      <td>(it, 114.00279976905662)</td>\n      <td>(like, 36.7672996472524)</td>\n      <td>(is, 55.94487331494873)</td>\n      <td>(like, 36.7672996472524)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>(it, 2562)</td>\n      <td>(ani, 505)</td>\n      <td>(in, 102.13673946564619)</td>\n      <td>(ha, 34.4237897180173)</td>\n      <td>(that, 48.31727363290116)</td>\n      <td>(ha, 34.4237897180173)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>(for, 2362)</td>\n      <td>(program, 497)</td>\n      <td>(that, 96.8675995748726)</td>\n      <td>(doe, 34.19333412527219)</td>\n      <td>(in, 47.30170083345097)</td>\n      <td>(doe, 34.19333412527219)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>(that, 2237)</td>\n      <td>(ha, 479)</td>\n      <td>(for, 90.52244749715135)</td>\n      <td>(just, 29.513176624612083)</td>\n      <td>(you, 45.68450447600251)</td>\n      <td>(just, 29.513176624612083)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>(you, 2086)</td>\n      <td>(edu, 468)</td>\n      <td>(you, 79.67798552554372)</td>\n      <td>(thank, 28.549402258522097)</td>\n      <td>(for, 43.47570804261264)</td>\n      <td>(thank, 28.549402258522097)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>(be, 1647)</td>\n      <td>(like, 457)</td>\n      <td>(be, 68.85460938269497)</td>\n      <td>(anyon, 28.298548977191338)</td>\n      <td>(be, 37.344622742852174)</td>\n      <td>(anyon, 28.298548977191338)</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>(thi, 1472)</td>\n      <td>(bit, 451)</td>\n      <td>(on, 61.11286152167876)</td>\n      <td>(work, 26.494787158165767)</td>\n      <td>(thi, 34.3491285954964)</td>\n      <td>(work, 26.494787158165767)</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>(on, 1469)</td>\n      <td>(format, 411)</td>\n      <td>(have, 60.90208541002496)</td>\n      <td>(think, 24.640467411932118)</td>\n      <td>(have, 34.04975283966364)</td>\n      <td>(think, 24.640467411932118)</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>(have, 1298)</td>\n      <td>(know, 401)</td>\n      <td>(thi, 60.34198459030236)</td>\n      <td>(need, 24.56412549445321)</td>\n      <td>(on, 33.209071780256735)</td>\n      <td>(need, 24.56412549445321)</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>(or, 1295)</td>\n      <td>(doe, 386)</td>\n      <td>(or, 50.6519636067793)</td>\n      <td>(program, 24.56261574830257)</td>\n      <td>(if, 29.438597446898267)</td>\n      <td>(program, 24.56261574830257)</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>(with, 1260)</td>\n      <td>(data, 369)</td>\n      <td>(if, 49.388084620385364)</td>\n      <td>(look, 24.441912389427163)</td>\n      <td>(or, 29.11817727731375)</td>\n      <td>(look, 24.441912389427163)</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>(are, 1212)</td>\n      <td>(onli, 344)</td>\n      <td>(with, 47.226534170832274)</td>\n      <td>(make, 24.08146082002033)</td>\n      <td>(can, 28.41866932660334)</td>\n      <td>(make, 24.08146082002033)</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>(if, 1154)</td>\n      <td>(work, 344)</td>\n      <td>(can, 46.08463643662672)</td>\n      <td>(key, 23.553337023165067)</td>\n      <td>(are, 28.194428691654128)</td>\n      <td>(key, 23.553337023165067)</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>(use, 1097)</td>\n      <td>(make, 341)</td>\n      <td>(are, 45.258192235058026)</td>\n      <td>(pleas, 23.374159220189213)</td>\n      <td>(do, 27.903939642519983)</td>\n      <td>(pleas, 23.374159220189213)</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>(not, 1077)</td>\n      <td>(just, 339)</td>\n      <td>(do, 44.57070053438678)</td>\n      <td>(onli, 22.125484501080784)</td>\n      <td>(not, 27.900212314318484)</td>\n      <td>(onli, 22.125484501080784)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_stem"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:04.865882500Z",
     "start_time": "2023-12-17T11:11:04.500728600Z"
    }
   },
   "id": "7110c29b6add8dd4"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('./../output/2lab_result.xlsx', engine='openpyxl')\n",
    "\n",
    "df_train.to_excel(writer, sheet_name='Train, wo stem')\n",
    "df_test.to_excel(writer, sheet_name='Test, wo stem')\n",
    "df_train_stem.to_excel(writer, sheet_name='Train, with stem')\n",
    "df_test_stem.to_excel(writer, sheet_name='Test, with stem')\n",
    "\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:04.866883100Z",
     "start_time": "2023-12-17T11:11:04.520661400Z"
    }
   },
   "id": "12abeaacee22ab56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "8. Конвеер"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "767adc78ce531af7"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best score: 0.893\n",
      "Best parameters set:\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'tfidf__use_idf': True,\n 'vect__max_features': 5000,\n 'vect__stop_words': 'english'}"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_features': (500, 1000, 2500, 5000, 10000, None),\n",
    "    'vect__stop_words': ('english', None),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(train_bunch.data, train_bunch.target)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "grid_search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:09.361022400Z",
     "start_time": "2023-12-17T11:11:04.560877Z"
    }
   },
   "id": "814e16d1cb9a59a3"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       593\n",
      "           1       0.96      0.92      0.94       621\n",
      "           2       0.92      0.98      0.95       556\n",
      "\n",
      "    accuracy                           0.95      1770\n",
      "   macro avg       0.95      0.95      0.95      1770\n",
      "weighted avg       0.95      0.95      0.95      1770\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(grid_search.predict(train_bunch.data), train_bunch.target))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:09.516664300Z",
     "start_time": "2023-12-17T11:11:09.336939600Z"
    }
   },
   "id": "29d71ade91f715e0"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.82      0.85       424\n",
      "           1       0.88      0.79      0.83       441\n",
      "           2       0.70      0.88      0.78       313\n",
      "\n",
      "    accuracy                           0.82      1178\n",
      "   macro avg       0.82      0.83      0.82      1178\n",
      "weighted avg       0.84      0.82      0.82      1178\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(grid_search.predict(test_bunch.data), test_bunch.target))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:09.647090900Z",
     "start_time": "2023-12-17T11:11:09.516664300Z"
    }
   },
   "id": "dfa271f8e1a09783"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Пайплайн для стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e149e57ae4fd495d"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best score: 0.886\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'tfidf__use_idf': True,\n 'vect__max_features': 5000,\n 'vect__stop_words': 'english'}"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_features': (500, 1000, 2500, 5000, 10000, None),\n",
    "    'vect__stop_words': ('english', None),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(train_tokenized, train_bunch.target)\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "grid_search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:13.204311500Z",
     "start_time": "2023-12-17T11:11:09.647090900Z"
    }
   },
   "id": "7359e09593b7d633"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       585\n",
      "           1       0.97      0.92      0.94       629\n",
      "           2       0.92      0.98      0.95       556\n",
      "\n",
      "    accuracy                           0.95      1770\n",
      "   macro avg       0.95      0.95      0.95      1770\n",
      "weighted avg       0.95      0.95      0.95      1770\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(grid_search.predict(train_tokenized), train_bunch.target))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:13.390358Z",
     "start_time": "2023-12-17T11:11:13.203311700Z"
    }
   },
   "id": "f023a6bf669d74a2"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.82      0.85       421\n",
      "           1       0.90      0.78      0.84       455\n",
      "           2       0.69      0.90      0.78       302\n",
      "\n",
      "    accuracy                           0.83      1178\n",
      "   macro avg       0.83      0.83      0.82      1178\n",
      "weighted avg       0.84      0.83      0.83      1178\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(grid_search.predict(test_tokenized), test_bunch.target))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-17T11:11:13.482202900Z",
     "start_time": "2023-12-17T11:11:13.391358400Z"
    }
   },
   "id": "7a54910785ebe873"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
