{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:07.746983800Z",
     "start_time": "2023-11-11T09:51:07.645977500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Загрузка обучающей и тестовой выборки"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d77a5ebb854176d"
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "\n",
    "def get_train_data(categories):\n",
    "    if type(categories) is not list:\n",
    "        categories = [categories]\n",
    "    return fetch_20newsgroups(subset='train', shuffle=True, categories=categories, random_state=42, remove=remove)\n",
    "\n",
    "\n",
    "all_categories = ['comp.graphics', 'sci.crypt', 'sci.electronics']\n",
    "train_bunch = get_train_data(all_categories)\n",
    "test_bunch = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, categories=all_categories, remove=remove)\n",
    "\n",
    "\n",
    "def get_sample(bunch, category_idx):\n",
    "    for idx, target in enumerate(bunch.target):\n",
    "        if target == category_idx:\n",
    "            return bunch.data[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:09.182788400Z",
     "start_time": "2023-11-11T09:51:07.652984300Z"
    }
   },
   "id": "9eae428c0f3ee6d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Вывод по одному документа каждого из классов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64f88417caaa408a"
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "data": {
      "text/plain": "\"Hello, I realize that this might be a FAQ but I have to ask since I don't get a\\nchange to read this newsgroup very often.  Anyways for my senior project I need\\nto convert an AutoCad file to a TIFF file.  Please I don't need anyone telling\\nme that the AutoCAD file is a vector file and the TIFF is a bit map since I\\nhave heard that about 100 times already I would just like to know if anyone\\nknows how to do this or at least point me to the right direction.\""
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample(train_bunch, all_categories.index('comp.graphics'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:09.184782800Z",
     "start_time": "2023-11-11T09:51:09.178774800Z"
    }
   },
   "id": "ac9222ee529c5311"
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "data": {
      "text/plain": "'Looking for PostScript or Tex version of a paper called:\\n\\t\"PUBLIC-KEY CRYPTOGRAPHY\"\\n\\nWritten by:\\n\\tJames Nechvatal\\n\\tSecurity Technology Group\\n\\tNational Computer Systems Laboratory\\n\\tNational Institute of Standards and Technology\\n\\tGaithersburg, MD 20899\\n\\n\\tDecember 1990\\n\\nThe version I obtained is plain text and all symbolic character\\nformatting has been lost.\\n'"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample(train_bunch, all_categories.index('sci.crypt'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:09.208953900Z",
     "start_time": "2023-11-11T09:51:09.184782800Z"
    }
   },
   "id": "95df8a2025d87bb9"
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "'Just a thought........Maybe it possibly has to do with the fact that it\\nIS an Emerson.  I\\'ve got an Emerson VCR which is #6 in the series.  Returned\\nit six times for various and never the same problems.  Got tired of taking it \\nback and fixed it myself.  The Hi-Fi \"window\" was a bit off.  Something like\\nthe Hi-Fi audio fine-tuning.  When I was a Wal-Mart \"associate\" in \\'88-\\'89,\\nwe had AT LEAST one returned as defective EVERY SINGLE DAY.  How\\'s that for\\nreliability?  Face it--Emerson can make audio stuff (albeit not of premium\\nquality), but they CAN\\'T make anything as complex as video equipment with \\nreliability IMHO.  Please, no flames.  Just *had* to share my Emerson disaster\\nin the light of this exploding tv.  \\nJC\\n\\n\\n'"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sample(train_bunch, all_categories.index('sci.electronics'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:09.260555700Z",
     "start_time": "2023-11-11T09:51:09.205953Z"
    }
   },
   "id": "b80e7a3e75954a9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Выполнение процедуры стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c355e8bef14d528"
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ruslan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import *\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "def stemminize(documents: list[str]) -> list[str]:\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stem_train = []\n",
    "    for document in documents:\n",
    "        nltk_tokens = word_tokenize(document)\n",
    "        line = ''\n",
    "        for word in nltk_tokens:\n",
    "            line += ' ' + porter_stemmer.stem(word)\n",
    "        stem_train.append(line)\n",
    "    return stem_train\n",
    "\n",
    "\n",
    "train_tokenized = stemminize(train_bunch.data)\n",
    "test_tokenized = stemminize(test_bunch.data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:16.479841700Z",
     "start_time": "2023-11-11T09:51:09.222554500Z"
    }
   },
   "id": "7b040729d22d126f"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "[\" hello , i realiz that thi might be a faq but i have to ask sinc i do n't get a chang to read thi newsgroup veri often . anyway for my senior project i need to convert an autocad file to a tiff file . pleas i do n't need anyon tell me that the autocad file is a vector file and the tiff is a bit map sinc i have heard that about 100 time alreadi i would just like to know if anyon know how to do thi or at least point me to the right direct .\",\n \" just a thought ........ mayb it possibl ha to do with the fact that it is an emerson . i 've got an emerson vcr which is # 6 in the seri . return it six time for variou and never the same problem . got tire of take it back and fix it myself . the hi-fi `` window '' wa a bit off . someth like the hi-fi audio fine-tun . when i wa a wal-mart `` associ '' in '88-'89 , we had at least one return as defect everi singl day . how 's that for reliabl ? face it -- emerson can make audio stuff ( albeit not of premium qualiti ) , but they ca n't make anyth as complex as video equip with reliabl imho . pleas , no flame . just * had * to share my emerson disast in the light of thi explod tv . jc\",\n \" it 's realli not that hard to do . there are book out there which explain everyth , and the basic 3d function , translat , rotat , shade , and hidden line remov are pretti easi . i wrote a program in a few week witht he help of a book , and would be happi to give you my sourc . also , quickdraw ha a lot of 3d function built in , and think pascal can access them , and i would expect that think c could as well . if you can find out how to use the quickdraw graphic librari , it would be an excel choic , sinc it ha a lot of stuff , and is built into the mac , so should be fast . libertarian , atheist , semi-anarch techno-rat .\"]"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вывод 3 первых документов обучающих данных\n",
    "train_tokenized[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:16.486086400Z",
     "start_time": "2023-11-11T09:51:16.481521600Z"
    }
   },
   "id": "82514f4345adc953"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "[' well , i am place a file at my ftp today that contain sever polygon descript of a head , face , skull , vase , etc . the format of the file is a list of vertic , normal , and triangl . there are variou resolut and the name of the data file includ the number of polygon , eg . phred.1.3k.vbl contain 1300 polygon . in order to get the data via ftp do the follow : 1 ) ftp taurus.cs.nps.navy.mil 2 ) login as anonym , guest as the password 3 ) cd pub/dabro 4 ) binari 5 ) get cyber.tar.z onc you get the data onto your workstat : 1 ) uncompress data.tar.z 2 ) tar xvof data.tar if you have ani question , pleas let me know . georg dabro dabro @ taurus.cs.nps.navy.mil -- georg dabrowski cyberwar lab',\n \" tri search for dmorf , i think it 's locat on wuarchive.wustl.edu in a mirror directori ... i 've use it befor , & it wa pretti good !\",\n ' not realli . i think it is less than 10 % .']"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вывод 3 первых документов тестовых данных\n",
    "test_tokenized[:3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:16.507648600Z",
     "start_time": "2023-11-11T09:51:16.484075100Z"
    }
   },
   "id": "6b4cec2df528fe76"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Векторизация и вывод 20 наиболее частых слов для всей тренировочной выборки без стоп-слов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "357cb1d9af7f853a"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 16689),\n ('to', 8883),\n ('of', 7021),\n ('and', 6843),\n ('is', 5467),\n ('in', 4416),\n ('it', 3900),\n ('that', 3682),\n ('for', 3677),\n ('you', 2852),\n ('be', 2788),\n ('this', 2585),\n ('on', 2451),\n ('are', 2155),\n ('with', 2111),\n ('or', 2090),\n ('have', 1879),\n ('as', 1784),\n ('can', 1704),\n ('if', 1702)]"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(max_features=10000)\n",
    "train_data = vect.fit_transform(train_bunch.data)\n",
    "\n",
    "\n",
    "def get_20_freq_words(vect, data):\n",
    "    words = list(zip(vect.get_feature_names_out(), np.ravel(data.sum(axis=0))))\n",
    "    words.sort(key=lambda x: x[1], reverse=True)\n",
    "    return words[:20]\n",
    "\n",
    "\n",
    "get_20_freq_words(vect, train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:16.675478700Z",
     "start_time": "2023-11-11T09:51:16.502650100Z"
    }
   },
   "id": "506302ae4212e71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Векторизация и вывод 20 наиболее частых слов для каждого класса тренировочной выборки по отдельности без стоп-слов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "576fe3d05418ae92"
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category=comp.graphics\n",
      "[('the', 3652), ('to', 2146), ('and', 1961), ('of', 1745), ('is', 1407), ('for', 1259), ('in', 1144), ('it', 1113), ('you', 859), ('that', 771), ('on', 728), ('this', 667), ('or', 601), ('with', 579), ('be', 568), ('can', 525), ('are', 514), ('have', 512), ('if', 498), ('from', 496)]\n",
      "category=sci.crypt\n",
      "[('the', 8980), ('to', 4739), ('of', 3888), ('and', 3506), ('is', 2797), ('in', 2232), ('that', 2108), ('it', 1865), ('be', 1655), ('for', 1565), ('this', 1365), ('on', 1150), ('are', 1090), ('you', 1085), ('with', 1010), ('as', 968), ('or', 955), ('not', 918), ('key', 906), ('have', 868)]\n",
      "category=sci.electronics\n",
      "[('the', 4057), ('to', 1998), ('of', 1388), ('and', 1376), ('is', 1263), ('in', 1040), ('it', 922), ('you', 908), ('for', 853), ('that', 803), ('on', 573), ('be', 565), ('this', 553), ('are', 551), ('or', 534), ('with', 522), ('have', 499), ('if', 477), ('as', 374), ('not', 371)]\n"
     ]
    }
   ],
   "source": [
    "for category in all_categories:\n",
    "    print(f\"category={category}\")\n",
    "    bunch = get_train_data(category)\n",
    "    vect = CountVectorizer(max_features=10000)\n",
    "    # dtm - Document Term Matrix\n",
    "    dtm = vect.fit_transform(bunch.data)\n",
    "    print(get_20_freq_words(vect, dtm))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:19.481250300Z",
     "start_time": "2023-11-11T09:51:16.675478700Z"
    }
   },
   "id": "e8aab3fd8050d022"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Векторизация и вывод 20 наиболее частых слов для всей тренировочной выборки со стоп-словами"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb9f331ad289ef96"
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "data": {
      "text/plain": "[('key', 937),\n ('use', 932),\n ('like', 642),\n ('don', 592),\n ('db', 562),\n ('edu', 553),\n ('encryption', 552),\n ('data', 547),\n ('know', 542),\n ('just', 533),\n ('chip', 521),\n ('does', 501),\n ('used', 498),\n ('information', 497),\n ('image', 492),\n ('people', 483),\n ('time', 447),\n ('bit', 437),\n ('file', 427),\n ('graphics', 423)]"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vect.fit_transform(train_bunch.data)\n",
    "\n",
    "get_20_freq_words(vect, dtm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:19.641143400Z",
     "start_time": "2023-11-11T09:51:19.507248400Z"
    }
   },
   "id": "19ab0f55db9071f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Векторизация и вывод 20 наиболее частых слов для каждого класса тренировочной со стоп-словами"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36bd00705f8a47fa"
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category=comp.graphics\n",
      "[('image', 484), ('graphics', 410), ('edu', 297), ('jpeg', 267), ('file', 265), ('use', 225), ('data', 219), ('files', 217), ('images', 212), ('software', 212), ('program', 199), ('ftp', 189), ('available', 185), ('format', 178), ('color', 174), ('like', 167), ('know', 165), ('pub', 161), ('gif', 160), ('does', 157)]\n",
      "category=sci.crypt\n",
      "[('key', 906), ('encryption', 551), ('db', 549), ('use', 448), ('chip', 438), ('government', 404), ('clipper', 387), ('people', 376), ('privacy', 349), ('keys', 340), ('security', 331), ('public', 313), ('information', 303), ('like', 285), ('just', 279), ('don', 271), ('law', 268), ('anonymous', 250), ('data', 246), ('used', 241)]\n",
      "category=sci.electronics\n",
      "[('use', 259), ('like', 190), ('power', 168), ('don', 166), ('wire', 163), ('ground', 161), ('used', 160), ('know', 148), ('does', 144), ('good', 142), ('circuit', 139), ('just', 136), ('current', 130), ('need', 120), ('wiring', 116), ('work', 115), ('time', 112), ('ve', 111), ('want', 111), ('output', 91)]\n"
     ]
    }
   ],
   "source": [
    "for category in all_categories:\n",
    "    print(f\"category={category}\")\n",
    "    bunch = get_train_data(category)\n",
    "    vect = CountVectorizer(max_features=10000, stop_words='english')\n",
    "    dtm = vect.fit_transform(bunch.data)\n",
    "    print(get_20_freq_words(vect, dtm))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:22.249229900Z",
     "start_time": "2023-11-11T09:51:19.643142300Z"
    }
   },
   "id": "510a4cc5568129c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Векторизация и вывод 20 наиболее частых слов для всей тестовой выборки без стоп-слов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a24b147f93be958e"
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "[('the', 9066),\n ('to', 5360),\n ('of', 4137),\n ('and', 4073),\n ('is', 3074),\n ('in', 2610),\n ('it', 2402),\n ('for', 2362),\n ('that', 2228),\n ('you', 2086),\n ('be', 1535),\n ('this', 1472),\n ('on', 1462),\n ('or', 1295),\n ('with', 1258),\n ('have', 1215),\n ('are', 1186),\n ('if', 1154),\n ('can', 1101),\n ('as', 1026)]"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000)\n",
    "dtm = vect.fit_transform(test_bunch.data)\n",
    "\n",
    "get_20_freq_words(vect, dtm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:22.357123Z",
     "start_time": "2023-11-11T09:51:22.250230700Z"
    }
   },
   "id": "d149dfdb9e61f15d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Векторизация и вывод 20 наиболее частых слов для каждого класса тестовой выборки без стоп-слов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2f63df1033682a5"
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category=comp.graphics\n",
      "[('the', 3694), ('to', 2376), ('and', 2208), ('of', 1945), ('is', 1505), ('for', 1351), ('in', 1275), ('you', 1053), ('it', 1045), ('that', 770), ('on', 734), ('this', 707), ('or', 681), ('image', 655), ('with', 655), ('be', 642), ('are', 580), ('can', 558), ('from', 547), ('jpeg', 526)]\n",
      "category=sci.crypt\n",
      "[('the', 3251), ('to', 1859), ('of', 1402), ('and', 1116), ('that', 915), ('is', 891), ('in', 801), ('it', 801), ('be', 577), ('for', 563), ('you', 553), ('this', 477), ('not', 423), ('on', 420), ('have', 418), ('if', 380), ('or', 359), ('are', 358), ('they', 355), ('with', 338)]\n",
      "category=sci.electronics\n",
      "[('the', 2121), ('to', 1125), ('of', 790), ('and', 749), ('is', 678), ('it', 556), ('that', 543), ('in', 534), ('you', 480), ('for', 448), ('be', 316), ('on', 308), ('have', 301), ('this', 288), ('with', 265), ('if', 262), ('or', 255), ('are', 248), ('can', 239), ('but', 236)]\n"
     ]
    }
   ],
   "source": [
    "def get_test_data(categories):\n",
    "    if type(categories) is not list:\n",
    "        categories = [categories]\n",
    "    return fetch_20newsgroups(subset='test', shuffle=True, random_state=42, categories=categories, remove=remove)\n",
    "\n",
    "\n",
    "for category in all_categories:\n",
    "    print(f\"category={category}\")\n",
    "    bunch = get_test_data(category)\n",
    "    vect = CountVectorizer(max_features=10000)\n",
    "    dtm = vect.fit_transform(bunch.data)\n",
    "    print(get_20_freq_words(vect, dtm))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:24.125684800Z",
     "start_time": "2023-11-11T09:51:22.359122400Z"
    }
   },
   "id": "52a6b03e4d340098"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Векторизация и вывод 20 наиболее частых слов для всей тестовой выборки со стоп-словами"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "781cbe631fdde412"
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "[('image', 666),\n ('jpeg', 526),\n ('use', 516),\n ('edu', 468),\n ('graphics', 462),\n ('like', 408),\n ('file', 389),\n ('don', 378),\n ('data', 368),\n ('know', 355),\n ('just', 339),\n ('bit', 337),\n ('available', 325),\n ('software', 324),\n ('images', 307),\n ('program', 298),\n ('does', 291),\n ('time', 282),\n ('used', 272),\n ('ftp', 271)]"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vect.fit_transform(test_bunch.data)\n",
    "\n",
    "get_20_freq_words(vect, dtm)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:24.229060100Z",
     "start_time": "2023-11-11T09:51:24.126684800Z"
    }
   },
   "id": "11288b5195cf9d2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Векторизация и вывод 20 наиболее частых слов для каждого класса тестовой выборки со стоп-словами"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d430e6d3c711f2"
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category=comp.graphics\n",
      "[('image', 655), ('jpeg', 526), ('graphics', 456), ('edu', 404), ('file', 366), ('images', 302), ('available', 269), ('format', 262), ('gif', 253), ('data', 249), ('ftp', 248), ('bit', 245), ('software', 245), ('color', 221), ('use', 218), ('files', 215), ('pub', 205), ('program', 197), ('version', 193), ('like', 188)]\n",
      "category=sci.crypt\n",
      "[('government', 214), ('key', 176), ('use', 176), ('clipper', 165), ('chip', 151), ('don', 141), ('people', 141), ('encryption', 134), ('like', 127), ('just', 121), ('time', 116), ('know', 113), ('phone', 111), ('think', 111), ('message', 108), ('keys', 100), ('algorithm', 97), ('law', 94), ('security', 93), ('used', 92)]\n",
      "category=sci.electronics\n",
      "[('use', 122), ('just', 106), ('know', 100), ('used', 95), ('like', 93), ('don', 89), ('battery', 84), ('does', 74), ('copy', 69), ('time', 69), ('think', 68), ('program', 65), ('need', 63), ('make', 60), ('ve', 60), ('sure', 55), ('power', 53), ('want', 53), ('software', 52), ('radio', 51)]\n"
     ]
    }
   ],
   "source": [
    "for category in all_categories:\n",
    "    print(f\"category={category}\")\n",
    "    bunch = get_test_data(category)\n",
    "    vect = CountVectorizer(max_features=10000, stop_words='english')\n",
    "    dtm = vect.fit_transform(bunch.data)\n",
    "    print(get_20_freq_words(vect, dtm))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:25.975129800Z",
     "start_time": "2023-11-11T09:51:24.230060300Z"
    }
   },
   "id": "cf1f3ab21c201368"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.d Векторизация и вывод 20 наиболее частых слов для всей тренировочной выборки без стоп-слов с применением стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ea2c80f93760cd0"
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 16688), ('to', 8883), ('of', 7021), ('and', 6843), ('is', 5549), ('in', 4419), ('it', 4191), ('that', 3692), ('for', 3677), ('be', 2998), ('you', 2852), ('thi', 2585), ('on', 2459), ('are', 2195), ('with', 2111), ('or', 2090), ('use', 2014), ('have', 1997), ('as', 1784), ('not', 1740)]\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000)\n",
    "dtm = vect.fit_transform(train_tokenized)\n",
    "print(get_20_freq_words(vect, dtm))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:26.131012900Z",
     "start_time": "2023-11-11T09:51:25.976130Z"
    }
   },
   "id": "3f67366012484802"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.d Векторизация и вывод 20 наиболее частых слов для каждого класса тренировочной выборки без стоп-слов с применением стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b8bd69364e2e7f0"
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category=comp.graphics\n",
      "[('the', 3651), ('to', 2146), ('and', 1961), ('of', 1745), ('is', 1419), ('for', 1259), ('it', 1200), ('in', 1146), ('you', 859), ('that', 771), ('on', 730), ('imag', 717), ('thi', 668), ('be', 604), ('or', 601), ('with', 579), ('have', 545), ('are', 522), ('use', 509), ('if', 498)]\n",
      "category=sci.crypt\n",
      "[('the', 8980), ('to', 4739), ('of', 3888), ('and', 3506), ('is', 2854), ('in', 2232), ('that', 2115), ('it', 2028), ('be', 1786), ('for', 1565), ('thi', 1364), ('key', 1249), ('on', 1154), ('are', 1118), ('you', 1085), ('with', 1010), ('not', 970), ('as', 968), ('use', 958), ('or', 955)]\n",
      "category=sci.electronics\n",
      "[('the', 4057), ('to', 1998), ('of', 1388), ('and', 1376), ('is', 1276), ('in', 1041), ('it', 963), ('you', 908), ('for', 853), ('that', 806), ('be', 608), ('on', 575), ('are', 555), ('thi', 553), ('use', 547), ('or', 534), ('have', 531), ('with', 522), ('if', 477), ('do', 401)]\n"
     ]
    }
   ],
   "source": [
    "for category in all_categories:\n",
    "    print(f\"category={category}\")\n",
    "    bunch = get_train_data(category)\n",
    "    stemminized = stemminize(bunch.data)\n",
    "    vect = CountVectorizer(max_features=10000)\n",
    "    dtm = vect.fit_transform(stemminized)\n",
    "    print(get_20_freq_words(vect, dtm))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:33.129150500Z",
     "start_time": "2023-11-11T09:51:26.127013500Z"
    }
   },
   "id": "ca2d5d0f0f8ccd50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.d Векторизация и вывод 20 наиболее частых слов для всей тестовой выборки со стоп-словами с применением стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7230a952e26793c1"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thi', 1472), ('use', 1097), ('imag', 998), ('file', 615), ('jpeg', 531), ('wa', 510), ('ani', 505), ('program', 497), ('ha', 479), ('edu', 468), ('like', 457), ('bit', 451), ('format', 411), ('know', 401), ('doe', 386), ('data', 369), ('onli', 344), ('work', 344), ('make', 341), ('just', 339)]\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vect.fit_transform(test_tokenized)\n",
    "print(get_20_freq_words(vect, dtm))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:33.225231500Z",
     "start_time": "2023-11-11T09:51:33.127152900Z"
    }
   },
   "id": "9caf481872d8d33d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.d Векторизация и вывод 20 наиболее частых слов для каждого класса тренировочной выборки cо стоп-словами с применением стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51c5eb29ac3150d4"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category=comp.graphics\n",
      "[('imag', 979), ('thi', 707), ('file', 580), ('jpeg', 531), ('use', 469), ('edu', 404), ('format', 395), ('program', 366), ('graphic', 321), ('bit', 309), ('color', 287), ('gif', 284), ('avail', 280), ('data', 250), ('ftp', 248), ('ani', 245), ('softwar', 234), ('display', 231), ('comput', 224), ('version', 224)]\n",
      "category=sci.crypt\n",
      "[('thi', 477), ('use', 354), ('key', 276), ('govern', 229), ('encrypt', 211), ('wa', 207), ('chip', 194), ('ha', 160), ('clipper', 155), ('ani', 152), ('like', 152), ('phone', 151), ('secur', 145), ('peopl', 140), ('know', 138), ('onli', 136), ('make', 126), ('just', 121), ('law', 121), ('think', 121)]\n",
      "category=sci.electronics\n",
      "[('thi', 288), ('use', 274), ('wa', 124), ('batteri', 113), ('ani', 108), ('just', 106), ('know', 106), ('ha', 104), ('like', 103), ('work', 100), ('doe', 99), ('make', 96), ('copi', 94), ('need', 85), ('program', 81), ('time', 75), ('anyon', 73), ('onli', 71), ('think', 71), ('board', 70)]\n"
     ]
    }
   ],
   "source": [
    "for category in all_categories:\n",
    "    print(f\"category={category}\")\n",
    "    bunch = get_test_data(category)\n",
    "    stemminized = stemminize(bunch.data)\n",
    "    vect = CountVectorizer(max_features=10000, stop_words='english')\n",
    "    dtm = vect.fit_transform(stemminized)\n",
    "    print(get_20_freq_words(vect, dtm))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:37.659899400Z",
     "start_time": "2023-11-11T09:51:33.225231500Z"
    }
   },
   "id": "fd1dfee25b61fe27"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью TfidfTransformer без стоп-слов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b3c57f6c668850c"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 214.27923011983643\n",
      "to: 128.50658234239415\n",
      "of: 101.86203264775209\n",
      "and: 91.733961272254\n",
      "is: 83.97727336110019\n",
      "it: 79.47554322700573\n",
      "in: 72.70953833589493\n",
      "that: 72.51290798766651\n",
      "for: 67.66475343892618\n",
      "you: 66.41180952122423\n",
      "be: 55.65313856163747\n",
      "this: 55.13173979874598\n",
      "on: 50.56554310512375\n",
      "have: 48.11707358740201\n",
      "are: 44.12242787539058\n",
      "with: 43.883460758799835\n",
      "if: 43.62474130570933\n",
      "or: 43.39770046917348\n",
      "can: 40.89942210826312\n",
      "as: 40.345366794020045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "dtm = vectorizer.fit_transform(train_bunch.data)\n",
    "tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "def get_20_freq_words_idf(feature_names, tfidf_values):\n",
    "    word_weights = dict(zip(feature_names, tfidf_values))\n",
    "    sorted_words = sorted(word_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, weight in sorted_words[:20]:\n",
    "        print(f\"{word}: {weight}\")\n",
    "        \n",
    "get_20_freq_words_idf(feature_names, tfidf_values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:37.868495100Z",
     "start_time": "2023-11-11T09:51:37.656899100Z"
    }
   },
   "id": "387def9a86f1221"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Векторизация и вывод 20 наиболее важных слов для каждого класса обучающей выборки с помощью TfidfTransformer без стоп-слов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2c7181f30c16241"
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category=comp.graphics\n",
      "the: 56.81603927198729\n",
      "to: 37.5504252863982\n",
      "of: 29.95573947448096\n",
      "and: 29.460733805639126\n",
      "is: 26.83975596124121\n",
      "it: 26.306797654818542\n",
      "for: 25.149437433894523\n",
      "in: 23.2357918510436\n",
      "you: 20.673734461421777\n",
      "that: 20.115182794971254\n",
      "on: 17.71246888444271\n",
      "this: 16.797040846507482\n",
      "have: 16.169172457895662\n",
      "be: 15.316347479606655\n",
      "or: 14.892040185365866\n",
      "can: 14.412387652558904\n",
      "if: 14.147115541575188\n",
      "with: 14.130815774612628\n",
      "any: 13.617128936651824\n",
      "but: 13.159431390155026\n",
      "None\n",
      "category=sci.crypt\n",
      "the: 99.93764847107565\n",
      "to: 57.66221081150154\n",
      "of: 46.1899156380585\n",
      "and: 38.59505532861134\n",
      "is: 34.65420251878092\n",
      "that: 33.583875491055075\n",
      "it: 31.869516519073688\n",
      "in: 29.482313408273818\n",
      "be: 26.4689599195368\n",
      "you: 24.889063304511872\n",
      "this: 24.376291130996346\n",
      "for: 23.58073764592061\n",
      "they: 20.05091740337681\n",
      "key: 19.51308036713041\n",
      "on: 18.340987575708446\n",
      "not: 18.18277256091116\n",
      "have: 17.90227365267764\n",
      "as: 17.80312531534647\n",
      "are: 17.670592540470366\n",
      "with: 16.98221081383814\n",
      "None\n",
      "category=sci.electronics\n",
      "the: 65.66425320736454\n",
      "to: 37.71142212609177\n",
      "of: 28.362427330724103\n",
      "and: 27.135787114635182\n",
      "is: 25.33387130387183\n",
      "it: 23.32173735332318\n",
      "you: 22.6974603506857\n",
      "in: 22.616468517675827\n",
      "for: 21.020490914802323\n",
      "that: 19.879884953712217\n",
      "on: 16.134278294163956\n",
      "have: 15.495513429313146\n",
      "this: 15.315744519468222\n",
      "are: 14.908597388785566\n",
      "be: 14.813968094668308\n",
      "if: 14.353220372094617\n",
      "with: 14.25180886861946\n",
      "or: 13.733645256306534\n",
      "there: 11.780219888716328\n",
      "as: 11.656328563354192\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for category in all_categories:\n",
    "    print(f\"category={category}\")\n",
    "    bunch = get_train_data(category)\n",
    "    vectorizer = CountVectorizer(max_features=10000)\n",
    "    dtm = vectorizer.fit_transform(bunch.data)\n",
    "    tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "    print(get_20_freq_words_idf(feature_names, tfidf_values))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:40.600027800Z",
     "start_time": "2023-11-11T09:51:37.869497800Z"
    }
   },
   "id": "6277dcff458f1dd2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью TfidfTransformer со стоп-словами"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b4e33c8b0b4951c"
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: 31.639250020331772\n",
      "know: 29.275985507770525\n",
      "use: 28.5909199903978\n",
      "like: 27.36974112627459\n",
      "does: 27.32020401912984\n",
      "don: 25.935560274661245\n",
      "just: 25.519485103509275\n",
      "chip: 24.453605471872685\n",
      "thanks: 24.11088770306822\n",
      "encryption: 20.612197002323146\n",
      "good: 20.345796034175873\n",
      "need: 19.637524126929815\n",
      "ve: 19.573641488242785\n",
      "graphics: 19.457067951393185\n",
      "clipper: 19.278143372470023\n",
      "people: 18.825771055423083\n",
      "think: 18.438571829249657\n",
      "used: 18.158442956203725\n",
      "government: 17.929344754138906\n",
      "time: 17.5444284746565\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(train_bunch.data)\n",
    "tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "get_20_freq_words_idf(feature_names, tfidf_values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:40.797863300Z",
     "start_time": "2023-11-11T09:51:40.607028400Z"
    }
   },
   "id": "f334676515430934"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Векторизация и вывод 20 наиболее важных слов для каждого класса обучающей выборки с помощью TfidfTransformer со стоп-словами"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7518b1791911af2"
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category=comp.graphics\n",
      "graphics: 13.831226949576145\n",
      "thanks: 12.288044819620865\n",
      "know: 11.28612771693418\n",
      "files: 10.102138516852015\n",
      "image: 10.059196059407899\n",
      "does: 10.004388990058809\n",
      "file: 9.554483758125274\n",
      "program: 9.124242438805213\n",
      "like: 8.74043723054119\n",
      "need: 8.575542354404014\n",
      "use: 8.551942822506788\n",
      "looking: 8.172372633378805\n",
      "don: 7.948725929628865\n",
      "help: 7.715196424156215\n",
      "windows: 7.512770455768797\n",
      "just: 7.431313142451506\n",
      "software: 7.336423321915559\n",
      "ve: 7.321258597928946\n",
      "format: 7.273424388627451\n",
      "hi: 7.226118442773505\n",
      "None\n",
      "category=sci.crypt\n",
      "key: 23.517928447854555\n",
      "chip: 16.042979038078702\n",
      "encryption: 16.009767020451324\n",
      "clipper: 14.966225678343589\n",
      "government: 13.886159272154273\n",
      "keys: 12.348914523675896\n",
      "people: 12.146083918114993\n",
      "use: 12.100090892961198\n",
      "just: 11.5722368986071\n",
      "don: 10.825121722527149\n",
      "nsa: 9.972883347244524\n",
      "know: 9.714097923177743\n",
      "like: 9.674787433122667\n",
      "does: 9.163514002454846\n",
      "public: 8.731548348649389\n",
      "escrow: 8.701773036575155\n",
      "security: 8.476496973427668\n",
      "think: 8.448380873436546\n",
      "law: 8.358591658515131\n",
      "phone: 8.34887878548174\n",
      "None\n",
      "category=sci.electronics\n",
      "like: 9.970259897533634\n",
      "use: 9.381693289952477\n",
      "know: 9.200738532630263\n",
      "does: 9.148565094352243\n",
      "don: 8.299237741326497\n",
      "power: 7.908172258790763\n",
      "good: 7.69009232573173\n",
      "thanks: 7.476689653329468\n",
      "used: 7.381491692228814\n",
      "just: 7.351913598461784\n",
      "ve: 7.1918846944034325\n",
      "ground: 6.355877580083519\n",
      "current: 6.3097482116905965\n",
      "want: 6.047200723878182\n",
      "work: 6.016294833755954\n",
      "circuit: 5.900250834752993\n",
      "need: 5.852806865041418\n",
      "output: 5.602228981788692\n",
      "number: 5.593040496699671\n",
      "help: 5.460062177920674\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for category in all_categories:\n",
    "    print(f\"category={category}\")\n",
    "    bunch = get_train_data(category)\n",
    "    vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(bunch.data)\n",
    "    tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "    print(get_20_freq_words_idf(feature_names, tfidf_values))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:43.577811200Z",
     "start_time": "2023-11-11T09:51:40.799863500Z"
    }
   },
   "id": "829f30b1489412e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью TfidfTransformer без стоп-слов c приминением стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ca589e9e50f780b"
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 214.47110783764484\n",
      "to: 128.91799924792693\n",
      "of: 101.9397995673158\n",
      "and: 91.85258731878157\n",
      "is: 84.95276457618122\n",
      "it: 82.95304153471035\n",
      "in: 73.08110129504044\n",
      "that: 73.01505717147099\n",
      "for: 68.08421571530532\n",
      "you: 66.66827307818957\n",
      "be: 59.109398696493265\n",
      "thi: 55.45567314375222\n",
      "on: 50.853304634787605\n",
      "have: 50.30994107364992\n",
      "are: 45.04857754230604\n",
      "with: 44.02554259499939\n",
      "if: 43.91731908930183\n",
      "or: 43.60061441479198\n",
      "do: 43.39462930056318\n",
      "use: 42.979583779815044\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "dtm = vectorizer.fit_transform(train_tokenized)\n",
    "tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "get_20_freq_words_idf(feature_names, tfidf_values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:43.802821800Z",
     "start_time": "2023-11-11T09:51:43.582483600Z"
    }
   },
   "id": "c3c5f382c3169075"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Векторизация и вывод 20 наиболее важных слов для каждого класса обучающей выборки с помощью TfidfTransformer без стоп-слов c приминением стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a2f9b9479340731"
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category=comp.graphics\n",
      "the: 57.84204157650781\n",
      "to: 38.340151343439004\n",
      "of: 30.585510691137237\n",
      "and: 30.045737608648373\n",
      "it: 28.000454544661952\n",
      "is: 27.41668278223464\n",
      "for: 25.715714722335886\n",
      "in: 23.779693685432704\n",
      "you: 21.137700831988884\n",
      "that: 20.533837250563355\n",
      "on: 18.1092841570932\n",
      "thi: 17.188796226050815\n",
      "have: 17.093136948410628\n",
      "be: 16.302077761169727\n",
      "do: 15.401428977570154\n",
      "or: 15.258422708993308\n",
      "file: 14.653988465757077\n",
      "if: 14.485327297878731\n",
      "with: 14.414977019404219\n",
      "ani: 14.03922847248981\n",
      "None\n",
      "category=sci.crypt\n",
      "the: 100.89753418910733\n",
      "to: 58.37532802365063\n",
      "of: 46.615240463294775\n",
      "and: 38.904579893384145\n",
      "is: 35.750875349520236\n",
      "that: 34.1864895509709\n",
      "it: 33.93805555595851\n",
      "in: 29.71597401845652\n",
      "be: 28.398975469667054\n",
      "key: 26.23362687401613\n",
      "you: 25.243952438346575\n",
      "thi: 24.69987466763479\n",
      "for: 23.866638227781138\n",
      "they: 20.298227761944975\n",
      "have: 18.960418087473922\n",
      "not: 18.883439176419014\n",
      "are: 18.5760104596733\n",
      "on: 18.540370636697027\n",
      "as: 17.944114308415344\n",
      "do: 17.490319647410548\n",
      "None\n",
      "category=sci.electronics\n",
      "the: 66.99167561855877\n",
      "to: 38.499351357977844\n",
      "of: 28.973971241587993\n",
      "and: 27.74259311411364\n",
      "is: 25.983178136354265\n",
      "it: 24.521028993961227\n",
      "in: 23.20428884952856\n",
      "you: 23.192774646818524\n",
      "for: 21.54620496148785\n",
      "that: 20.319824140462003\n",
      "on: 16.55360647095035\n",
      "have: 16.52081408392039\n",
      "be: 15.957971209293527\n",
      "thi: 15.711434983691749\n",
      "are: 15.338631615899397\n",
      "if: 14.698100718216187\n",
      "with: 14.56743461568831\n",
      "use: 14.38225380399237\n",
      "or: 14.063655744058696\n",
      "do: 12.505998953366\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for category in all_categories:\n",
    "    print(f\"category={category}\")\n",
    "    bunch = get_train_data(category)\n",
    "    stemminized = stemminize(bunch.data)\n",
    "    vectorizer = CountVectorizer(max_features=10000)\n",
    "    dtm = vectorizer.fit_transform(stemminized)\n",
    "    tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "    print(get_20_freq_words_idf(feature_names, tfidf_values))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:50.912428200Z",
     "start_time": "2023-11-11T09:51:43.798822800Z"
    }
   },
   "id": "a74172a0125f253e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Векторизация и вывод 20 наиболее важных слов для всей обучающей выборки с помощью TfidfTransformer со стоп-словами c приминением стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3742b44699994c6"
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: 31.639250020331772\n",
      "know: 29.275985507770525\n",
      "use: 28.5909199903978\n",
      "like: 27.36974112627459\n",
      "does: 27.32020401912984\n",
      "don: 25.935560274661245\n",
      "just: 25.519485103509275\n",
      "chip: 24.453605471872685\n",
      "thanks: 24.11088770306822\n",
      "encryption: 20.612197002323146\n",
      "good: 20.345796034175873\n",
      "need: 19.637524126929815\n",
      "ve: 19.573641488242785\n",
      "graphics: 19.457067951393185\n",
      "clipper: 19.278143372470023\n",
      "people: 18.825771055423083\n",
      "think: 18.438571829249657\n",
      "used: 18.158442956203725\n",
      "government: 17.929344754138906\n",
      "time: 17.5444284746565\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(train_bunch.data)\n",
    "tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "\n",
    "get_20_freq_words_idf(feature_names, tfidf_values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:51.117794500Z",
     "start_time": "2023-11-11T09:51:50.912428200Z"
    }
   },
   "id": "6f27e8699456e129"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Векторизация и вывод 20 наиболее важных слов для каждого класса обучающей выборки с помощью TfidfTransformer со стоп-словами c приминением стемминга"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70a2b67c9270ee7"
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category=comp.graphics\n",
      "thi: 19.621420625347906\n",
      "file: 16.61883229657882\n",
      "ani: 15.857764195559623\n",
      "use: 15.719093075820448\n",
      "thank: 13.746459600916015\n",
      "imag: 13.744728479290567\n",
      "know: 12.652182674316517\n",
      "program: 12.624755324724674\n",
      "graphic: 12.110970401672128\n",
      "look: 11.313449104830836\n",
      "doe: 11.094976683451524\n",
      "anyon: 10.551388274388568\n",
      "wa: 10.481184104117553\n",
      "need: 9.814535800027842\n",
      "help: 9.561833576753715\n",
      "format: 9.455752939643002\n",
      "pleas: 9.026265141642927\n",
      "point: 9.025544542011133\n",
      "ha: 8.966125867615792\n",
      "like: 8.901510963441291\n",
      "None\n",
      "category=sci.crypt\n",
      "key: 31.228292988781\n",
      "thi: 29.491860750296926\n",
      "encrypt: 20.8031575119436\n",
      "use: 20.343046535829462\n",
      "chip: 19.417298714056788\n",
      "clipper: 14.752755580075826\n",
      "govern: 14.217654249151735\n",
      "secur: 13.447096869841445\n",
      "ha: 13.34610828917708\n",
      "wa: 12.629522591032194\n",
      "peopl: 12.087664658455159\n",
      "doe: 11.902258153879002\n",
      "phone: 11.854376646657045\n",
      "bit: 11.802744216419127\n",
      "just: 11.549004341432651\n",
      "ani: 11.497761624127971\n",
      "like: 11.30424933095508\n",
      "escrow: 10.750788646668207\n",
      "know: 10.717659977601896\n",
      "onli: 10.128862478843875\n",
      "None\n",
      "category=sci.electronics\n",
      "thi: 17.680568670796706\n",
      "use: 16.067696294888083\n",
      "wa: 11.803884266632217\n",
      "ani: 11.785514926419097\n",
      "like: 10.655915456369128\n",
      "doe: 10.45510118929468\n",
      "know: 10.051393320301575\n",
      "anyon: 9.785231112866494\n",
      "work: 9.266609308174466\n",
      "power: 8.706840047590294\n",
      "ha: 8.647373963599035\n",
      "thank: 8.3956934930771\n",
      "good: 7.82344930056156\n",
      "ground: 7.823202818392092\n",
      "look: 7.7765386750979\n",
      "line: 7.7518726241854425\n",
      "circuit: 7.6610203688285665\n",
      "chip: 7.5360481268903845\n",
      "just: 7.530196927890259\n",
      "need: 7.298342167066325\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for category in all_categories:\n",
    "    print(f\"category={category}\")\n",
    "    bunch = get_train_data(category)\n",
    "    stemminized = stemminize(bunch.data)\n",
    "    vectorizer = CountVectorizer(max_features=10000, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform(stemminized)\n",
    "    tfidf = TfidfTransformer(use_idf=True).fit_transform(dtm)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_values = tfidf.toarray().sum(axis=0)\n",
    "    print(get_20_freq_words_idf(feature_names, tfidf_values))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-11T09:51:58.246949500Z",
     "start_time": "2023-11-11T09:51:51.117794500Z"
    }
   },
   "id": "bd7865b62eade329"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
